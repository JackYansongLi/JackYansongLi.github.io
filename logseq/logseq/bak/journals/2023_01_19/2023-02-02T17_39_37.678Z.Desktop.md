- TODO Read [[Convergence of Q learning]]
	- Borkar: Sean Meyn (RL Textbook) [[@Control Systems and Reinforcement Learning]] (ODE method) /zotero
	- Srikant(UIUC): Simons workshop talk:[https://simons.berkeley.edu/workshops/data-driven-decision-processes-boot-camp/schedule#simons-tabs](https://simons.berkeley.edu/workshops/data-driven-decision-processes-boot-camp/schedule#simons-tabs)
	- [[@Bandit Algorithms]] (Tor Lattimore and Csaba Szepesvari)
		- Chapter 38: Markov Decision Processes
			- Understand repeated game vs general
			- Understand MAB vs MDP
	- [[@Individual Q-Learning in Normal Form Games]]:
		- Convergence in what sense?
			- Answer: Converge to Nash Distribution not Nash equilibrium
		- Go through the proof in detail
		- Do some numerical test
-
- TODO Implementation
	- Q-learning + 1D random walk
		- n_step vs problem size
	- Check $P$ for 2D
		- Write down $P$ mathematically
		- Check code implementation
	- Implement TD-learning algorithm along with a uniform-selection policy
		- Compare the learnt V function with the result gained from policy evaluation method that uses the model information