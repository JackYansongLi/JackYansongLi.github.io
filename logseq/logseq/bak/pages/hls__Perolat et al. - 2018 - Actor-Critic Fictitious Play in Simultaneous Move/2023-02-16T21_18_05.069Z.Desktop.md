file:: [Perolat et al. - 2018 - Actor-Critic Fictitious Play in Simultaneous Move .pdf](file://C:/Users/Haggi/Zotero/storage/R3I5XNRP/Perolat et al. - 2018 - Actor-Critic Fictitious Play in Simultaneous Move .pdf)
file-path:: file://C:/Users/Haggi/Zotero/storage/R3I5XNRP/Perolat et al. - 2018 - Actor-Critic Fictitious Play in Simultaneous Move .pdf

- there exists a three-player normal form game [15] for which no first order uncoupled dynamics (i.e. most decentralized dynamics) can converge to a Nash equilibrium. 
  ls-type:: annotation
  hl-page:: 2
  hl-color:: yellow
  id:: 63ed4bb4-b7ac-4a8c-804c-d9b50f50b09b
- On-line algorithms like Q-learning [32] are often used in cooperative multi-agent learning environments but fail to learn a stationary strategy in simultaneous zero-sum two-player games. 
  ls-type:: annotation
  hl-page:: 2
  hl-color:: yellow
  id:: 63ed4e9b-67e1-4a7d-a5a7-4f39d5cc5202
- n [25], the Q-learning method is adapted to guarantee convergence to zero-sum two-player MGs. This method isnâ€™t an independent learning algorithm anymore as each player needs to observe the action of the opponent. 
  ls-type:: annotation
  hl-page:: 2
  hl-color:: yellow
  id:: 63ed4f7d-ceef-4981-85b1-929da696795f