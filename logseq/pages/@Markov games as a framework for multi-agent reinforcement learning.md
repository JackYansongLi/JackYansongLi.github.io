date:: 1994
publisher:: Elsevier
extra:: DOI: 10.1016/B978-1-55860-335-6.50027-1
isbn:: 978-1-55860-335-6
title:: @Markov games as a framework for multi-agent reinforcement learning
book-title:: Machine Learning Proceedings 1994
pages:: 157-163
item-type:: [[bookSection]]
access-date:: 2022-10-27T20:15:35Z
original-title:: Markov games as a framework for multi-agent reinforcement learning
language:: en
url:: https://linkinghub.elsevier.com/retrieve/pii/B9781558603356500271
authors:: [[Michael L. Littman]]
library-catalog:: DOI.org (Crossref)
links:: [Local library](zotero://select/library/items/H6J9KRDV), [Web library](https://www.zotero.org/users/7448055/items/H6J9KRDV)

- [[Abstract]]
	- In the Markov decision process (MDP) formalization of reinforcement learning, a single adaptive agent interacts with an environment deﬁned by a probabilistic transition function. In this solipsistic view, secondary agents can only be part of the environment and are therefore ﬁxed in their behavior. The framework of Markov games allows us to widen this view to include multiple adaptive agents with interacting or competing goals. This paper considers a step in this direction in which exactly two agents with diametrically opposed goals share an environment. It describes a Q-learning-like algorithm for ﬁnding optimal policies and demonstrates its application to a simple two-player game in which the optimal policy is probabilistic.
- [[Attachments]]
	- [Littman - 1994 - Markov games as a framework for multi-agent reinfo.pdf](https://courses.cs.duke.edu/spring07/cps296.3/littman94markov.pdf) {{zotero-imported-file EZUDYSDX, "Littman - 1994 - Markov games as a framework for multi-agent reinfo.pdf"}}