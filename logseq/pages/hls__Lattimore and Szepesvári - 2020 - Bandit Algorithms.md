file:: [Lattimore and Szepesvári - 2020 - Bandit Algorithms.pdf](file://C:/Users/Haggi/Zotero/storage/DL6RWNHA/Lattimore and Szepesvári - 2020 - Bandit Algorithms.pdf)
file-path:: file://C:/Users/Haggi/Zotero/storage/DL6RWNHA/Lattimore and Szepesvári - 2020 - Bandit Algorithms.pdf

- This policy is then implemented until the number of visits to some state-action pair doubles when a new phase starts and the process begins again. The use of phases is important, not just for computational efficiency. Recalculating the optimistic policy in each round may lead to a dithering behaviour in which the algorithm frequently changes its plan and suffers linear regret (Exercise 38.19).
  ls-type:: annotation
  hl-page:: 532
  hl-color:: yellow
  id:: 63d1e7b2-b4e7-43c6-ba21-f2ccaa349906