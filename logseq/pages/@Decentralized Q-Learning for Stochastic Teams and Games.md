tags:: [[Control systems]], [[Convergence]], [[Decentralized control]], [[Games]], [[Heuristic algorithms]], [[Markov processes]], [[Standards]], [[learning in games]], [[stochastic dynamic teams]], [[stochastic games]]
date:: 2017-04
issn:: 1558-2523
issue:: 4
extra:: Conference Name: IEEE Transactions on Automatic Control
doi:: 10.1109/TAC.2016.2598476
title:: @Decentralized Q-Learning for Stochastic Teams and Games
pages:: 1545-1558
volume:: 62
item-type:: [[journalArticle]]
original-title:: Decentralized Q-Learning for Stochastic Teams and Games
publication-title:: IEEE Transactions on Automatic Control
authors:: [[Gürdal Arslan]], [[Serdar Yüksel]]
library-catalog:: IEEE Xplore
links:: [Local library](zotero://select/library/items/MQVAB9AW), [Web library](https://www.zotero.org/users/7448055/items/MQVAB9AW)

- [[Abstract]]
	- There are only a few learning algorithms applicable to stochastic dynamic teams and games which generalize Markov decision processes to decentralized stochastic control problems involving possibly self-interested decision makers. Learning in games is generally difficult because of the non-stationary environment in which each decision maker aims to learn its optimal decisions with minimal information in the presence of the other decision makers who are also learning. In stochastic dynamic games, learning is more challenging because, while learning, the decision makers alter the state of the system and hence the future cost. In this paper, we present decentralized Q-learning algorithms for stochastic games, and study their convergence for the weakly acyclic case which includes team problems as an important special case. The algorithms are decentralized in that each decision maker has access only to its own decisions and cost realizations as well as the state transitions; in particular, each decision maker is completely oblivious to the presence of the other decision makers. We show that these algorithms converge to equilibrium policies almost surely in large classes of stochastic games.
- [[Attachments]]
	- [IEEE Xplore Abstract Record](https://ieeexplore.ieee.org/document/7534842/?arnumber=7534842) {{zotero-imported-file 23AVCX4B, "7534842.html"}}
	- [IEEE Xplore Full Text PDF](https://ieeexplore.ieee.org/stampPDF/getPDF.jsp?tp=&arnumber=7534842&ref=aHR0cHM6Ly9pZWVleHBsb3JlLmllZWUub3JnL3N0YW1wL3N0YW1wLmpzcD90cD0mYXJudW1iZXI9NzUzNDg0Mg==) {{zotero-imported-file ZVV7QYYH, "Arslan and Yüksel - 2017 - Decentralized Q-Learning for Stochastic Teams and .pdf"}}
- ## Notes
	- Issues about centralized MARL algorithm [[Centralized MARL]]: ((63c99471-0771-409e-b948-c92b01334512))
	- ((63c99587-8c15-437f-841f-9ed018119692))
		- [8], [[@The Dynamics of Reinforcement Learning in Cooperative Multiagent Systems]]
	- ((63c9966e-5a0e-476a-b8b4-9676497b12ed))
	-
	-