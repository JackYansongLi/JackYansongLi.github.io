date:: [[May 31st, 2022]]
publisher:: Cambridge University Press
extra:: DOI: 10.1017/9781009051873
isbn:: 978-1-00-905187-3 978-1-316-51196-1
title:: @Control Systems and Reinforcement Learning
item-type:: [[book]]
access-date:: 2023-01-23T21:41:08Z
original-title:: Control Systems and Reinforcement Learning
url:: https://www.cambridge.org/core/product/identifier/9781009051873/type/book
edition:: 1
authors:: [[Sean Meyn]]
library-catalog:: DOI.org (Crossref)
links:: [Local library](zotero://select/library/items/Y7XSRGJC), [Web library](https://www.zotero.org/users/7448055/items/Y7XSRGJC)

- [[Abstract]]
	- A high school student can create deep Q-learning code to control her robot, without any understanding of the meaning of 'deep' or 'Q', or why the code sometimes fails. This book is designed to explain the science behind reinforcement learning and optimal control in a way that is accessible to students with a background in calculus and matrix algebra. A unique focus is algorithm design to obtain the fastest possible speed of convergence for learning algorithms, along with insight into why reinforcement learning sometimes fails. Advanced stochastic process theory is avoided at the start by substituting random exploration with more intuitive deterministic probing for learning. Once these ideas are understood, it is not difficult to master techniques rooted in stochastic control. These topics are covered in the second part of the book, starting with Markov chain theory and ending with a fresh look at actor-critic methods for reinforcement learning.
- [[Attachments]]
	- [CSRLonline.pdf](zotero://select/library/items/PEVGQHVC) {{zotero-imported-file PEVGQHVC, "CSRLonline.pdf"}}
- ## Chapter 9
	- ((63cf0c26-7131-40b9-b417-e79ef3da4fd8)) The difference between SARSA and Q-learning is that SARSA is on-policy while Q-learning is off-policy. TD learning learns the action value function instead of Q function. However, in this book, the author do not distinguish TD and SARSA.
	- ((63d2cdef-649d-4f79-ada1-51348d2d4bbf))
	-