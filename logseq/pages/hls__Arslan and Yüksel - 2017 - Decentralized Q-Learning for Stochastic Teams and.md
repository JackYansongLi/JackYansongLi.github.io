file:: [Arslan and Yüksel - 2017 - Decentralized Q-Learning for Stochastic Teams and .pdf](file:///Users/yansongli/zotero/storage/ZVV7QYYH/Arslan and Yüksel - 2017 - Decentralized Q-Learning for Stochastic Teams and .pdf)
file-path:: file://C:/Users/Haggi/Zotero/storage/ZVV7QYYH/Arslan and Yüksel - 2017 - Decentralized Q-Learning for Stochastic Teams and .pdf

- In many of these publications, the authors tend to assume that the real objective of the agents1 is for some reason to find and play an equilibrium strategy (and sometimes this even requires agents to somehow agree on a particular equilibrium strategy), and not necessarily to pursue their own objectives. Another serious issue is that the multi-agent algorithms introduced in many of these recent papers are not scalable since each agent needs to maintain estimates of its Q-factors for each state/joint action pair and compute an equilibrium at each step of the algorithm using the updated estimates, assuming that the actions and objectives are exchanged between all agents.
  ls-type:: annotation
  hl-page:: 1
  hl-color:: yellow
  id:: 63c99471-0771-409e-b948-c92b01334512
- The algorithms presented in [8] are claimed to be convergent to an equilibrium in single-state single-stage common interest games but without a proof.
  ls-type:: annotation
  hl-page:: 1
  hl-color:: yellow
  id:: 63c99587-8c15-437f-841f-9ed018119692
- FP algorithm is not convergent even in the simplest 2 x 2 x 2 case where there are two states and two agents each with two moves.
  ls-type:: annotation
  hl-page:: 2
  hl-color:: yellow
  id:: 63c9966e-5a0e-476a-b8b4-9676497b12ed