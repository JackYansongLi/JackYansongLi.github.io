@article{albrecht_belief_2016,
  title = {Belief and {{Truth}} in {{Hypothesised Behaviours}}},
  author = {Albrecht, Stefano V. and Crandall, Jacob W. and Ramamoorthy, Subramanian},
  year = {2016},
  month = jun,
  journal = {Artificial Intelligence},
  volume = {235},
  eprint = {1507.07688},
  primaryclass = {cs},
  pages = {63--94},
  issn = {00043702},
  doi = {10.1016/j.artint.2016.02.004},
  urldate = {2023-04-30},
  abstract = {There is a long history in game theory on the topic of Bayesian or "rational" learning, in which each player maintains beliefs over a set of alternative behaviours, or types, for the other players. This idea has gained increasing interest in the artificial intelligence (AI) community, where it is used as a method to control a single agent in a system composed of multiple agents with unknown behaviours. The idea is to hypothesise a set of types, each specifying a possible behaviour for the other agents, and to plan our own actions with respect to those types which we believe are most likely, given the observed actions of the agents. The game theory literature studies this idea primarily in the context of equilibrium attainment. In contrast, many AI applications have a focus on task completion and payoff maximisation. With this perspective in mind, we identify and address a spectrum of questions pertaining to belief and truth in hypothesised types. We formulate three basic ways to incorporate evidence into posterior beliefs and show when the resulting beliefs are correct, and when they may fail to be correct. Moreover, we demonstrate that prior beliefs can have a significant impact on our ability to maximise payoffs in the long-term, and that they can be computed automatically with consistent performance effects. Furthermore, we analyse the conditions under which we are able complete our task optimally, despite inaccuracies in the hypothesised types. Finally, we show how the correctness of hypothesised types can be ascertained during the interaction via an automated statistical analysis.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computer Science and Game Theory,I.2.11},
  file = {/Users/yansongli/Dropbox/zotero/storage/Y7VXWSQA/Albrecht et al. - 2016 - Belief and Truth in Hypothesised Behaviours.pdf;/Users/yansongli/Dropbox/zotero/storage/KEN4KWIM/1507.html}
}

@misc{albrecht_e-hba_2019,
  title = {E-{{HBA}}: {{Using Action Policies}} for {{Expert Advice}} and {{Agent Typification}}},
  shorttitle = {E-{{HBA}}},
  author = {Albrecht, Stefano V. and Crandall, Jacob W. and Ramamoorthy, Subramanian},
  year = {2019},
  month = jul,
  number = {arXiv:1907.09810},
  eprint = {1907.09810},
  primaryclass = {cs},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.1907.09810},
  urldate = {2023-04-30},
  abstract = {Past research has studied two approaches to utilise predefined policy sets in repeated interactions: as experts, to dictate our own actions, and as types, to characterise the behaviour of other agents. In this work, we bring these complementary views together in the form of a novel meta-algorithm, called Expert-HBA (E-HBA), which can be applied to any expert algorithm that considers the average (or total) payoff an expert has yielded in the past. E-HBA gradually mixes the past payoff with a predicted future payoff, which is computed using the type-based characterisation. We present results from a comprehensive set of repeated matrix games, comparing the performance of several well-known expert algorithms with and without the aid of E-HBA. Our results show that E-HBA has the potential to significantly improve the performance of expert algorithms.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Multiagent Systems},
  file = {/Users/yansongli/Dropbox/zotero/storage/2DIRZG4V/Albrecht et al. - 2019 - E-HBA Using Action Policies for Expert Advice and.pdf;/Users/yansongli/Dropbox/zotero/storage/WL45CNNI/1907.html}
}

@misc{albrecht_empirical_2019,
  title = {An {{Empirical Study}} on the {{Practical Impact}} of {{Prior Beliefs}} over {{Policy Types}}},
  author = {Albrecht, Stefano V. and Crandall, Jacob W. and Ramamoorthy, Subramanian},
  year = {2019},
  month = jul,
  number = {arXiv:1907.05247},
  eprint = {1907.05247},
  primaryclass = {cs},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.1907.05247},
  urldate = {2023-04-30},
  abstract = {Many multiagent applications require an agent to learn quickly how to interact with previously unknown other agents. To address this problem, researchers have studied learning algorithms which compute posterior beliefs over a hypothesised set of policies, based on the observed actions of the other agents. The posterior belief is complemented by the prior belief, which specifies the subjective likelihood of policies before any actions are observed. In this paper, we present the first comprehensive empirical study on the practical impact of prior beliefs over policies in repeated interactions. We show that prior beliefs can have a significant impact on the long-term performance of such methods, and that the magnitude of the impact depends on the depth of the planning horizon. Moreover, our results demonstrate that automatic methods can be used to compute prior beliefs with consistent performance effects. This indicates that prior beliefs could be eliminated as a manual parameter and instead be computed automatically.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Multiagent Systems},
  file = {/Users/yansongli/Dropbox/zotero/storage/SC7IAB7K/Albrecht et al. - 2019 - An Empirical Study on the Practical Impact of Prio.pdf;/Users/yansongli/Dropbox/zotero/storage/NVMAAEAC/1907.html}
}

@misc{albrecht_game-theoretic_2015,
  title = {A {{Game-Theoretic Model}} and {{Best-Response Learning Method}} for {{Ad Hoc Coordination}} in {{Multiagent Systems}}},
  author = {Albrecht, Stefano V. and Ramamoorthy, Subramanian},
  year = {2015},
  month = jun,
  number = {arXiv:1506.01170},
  eprint = {1506.01170},
  primaryclass = {cs},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.1506.01170},
  urldate = {2023-04-30},
  abstract = {The ad hoc coordination problem is to design an autonomous agent which is able to achieve optimal flexibility and efficiency in a multiagent system with no mechanisms for prior coordination. We conceptualise this problem formally using a game-theoretic model, called the stochastic Bayesian game, in which the behaviour of a player is determined by its private information, or type. Based on this model, we derive a solution, called Harsanyi-Bellman Ad Hoc Coordination (HBA), which utilises the concept of Bayesian Nash equilibrium in a planning procedure to find optimal actions in the sense of Bellman optimal control. We evaluate HBA in a multiagent logistics domain called level-based foraging, showing that it achieves higher flexibility and efficiency than several alternative algorithms. We also report on a human-machine experiment at a public science exhibition in which the human participants played repeated Prisoner's Dilemma and Rock-Paper-Scissors against HBA and alternative algorithms, showing that HBA achieves equal efficiency and a significantly higher welfare and winning rate.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computer Science and Game Theory,Computer Science - Multiagent Systems},
  file = {/Users/yansongli/Dropbox/zotero/storage/BKEGCN6F/Albrecht and Ramamoorthy - 2015 - A Game-Theoretic Model and Best-Response Learning .pdf;/Users/yansongli/Dropbox/zotero/storage/CYAM46GI/1506.html}
}

@misc{astrom_dual_1986,
  title = {Minimum {{Coverage Sets}} for {{Training Robust Ad Hoc Teamwork Agents}}},
  author = {Rahman, Arrasy and Cui, Jiaxun and Stone, Peter},
  year = {2023},
  month = aug,
  number = {arXiv:2308.09595},
  eprint = {2308.09595},
  primaryclass = {cs},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.2308.09595},
  urldate = {2023-11-09},
  abstract = {Robustly cooperating with unseen agents and human partners presents significant challenges due to the diverse cooperative conventions these partners may adopt. Existing Ad Hoc Teamwork (AHT) methods address this challenge by training an agent with a population of diverse teammate policies obtained through maximizing specific diversity metrics. However, these heuristic diversity metrics do not always maximize the agent's robustness in all cooperative problems. In this work, we first propose that maximizing an AHT agent's robustness requires it to emulate policies in the minimum coverage set (MCS), the set of best-response policies to any partner policies in the environment. We then introduce the L-BRDiv algorithm that generates a set of teammate policies that, when used for AHT training, encourage agents to emulate policies from the MCS. L-BRDiv works by solving a constrained optimization problem to jointly train teammate policies for AHT training and approximating AHT agent policies that are members of the MCS. We empirically demonstrate that L-BRDiv produces more robust AHT agents than state-of-the-art methods in a broader range of two-player cooperative problems without the need for extensive hyperparameter tuning for its objectives. Our study shows that L-BRDiv outperforms the baseline methods by prioritizing discovering distinct members of the MCS instead of repeatedly finding redundant policies.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Artificial Intelligence},
  file = {/Users/yansongli/Dropbox/zotero/storage/AXSHYA3F/Rahman et al. - 2023 - Minimum Coverage Sets for Training Robust Ad Hoc T.pdf;/Users/yansongli/Dropbox/zotero/storage/H2PC5K6K/2308.html}
}

@misc{ayoub_model-based_2020,
  title = {Model-{{Based Reinforcement Learning}} with {{Value-Targeted Regression}}},
  author = {Ayoub, Alex and Jia, Zeyu and Szepesvari, Csaba and Wang, Mengdi and Yang, Lin F.},
  year = {2020},
  month = jun,
  number = {arXiv:2006.01107},
  eprint = {2006.01107},
  primaryclass = {cs, stat},
  publisher = {{arXiv}},
  urldate = {2023-11-27},
  abstract = {This paper studies model-based reinforcement learning (RL) for regret minimization. We focus on finite-horizon episodic RL where the transition model \$P\$ belongs to a known family of models \${\textbackslash}mathcal\{P\}\$, a special case of which is when models in \${\textbackslash}mathcal\{P\}\$ take the form of linear mixtures: \$P\_\{{\textbackslash}theta\} = {\textbackslash}sum\_\{i=1\}\^\{d\} {\textbackslash}theta\_\{i\}P\_\{i\}\$. We propose a model based RL algorithm that is based on optimism principle: In each episode, the set of models that are `consistent' with the data collected is constructed. The criterion of consistency is based on the total squared error of that the model incurs on the task of predicting {\textbackslash}emph\{values\} as determined by the last value estimate along the transitions. The next value function is then chosen by solving the optimistic planning problem with the constructed set of models. We derive a bound on the regret, which, in the special case of linear mixtures, the regret bound takes the form \${\textbackslash}tilde\{{\textbackslash}mathcal\{O\}\}(d{\textbackslash}sqrt\{H\^\{3\}T\})\$, where \$H\$, \$T\$ and \$d\$ are the horizon, total number of steps and dimension of \${\textbackslash}theta\$, respectively. In particular, this regret bound is independent of the total number of states or actions, and is close to a lower bound \${\textbackslash}Omega({\textbackslash}sqrt\{HdT\})\$. For a general model family \${\textbackslash}mathcal\{P\}\$, the regret bound is derived using the notion of the so-called Eluder dimension proposed by Russo \& Van Roy (2014).},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/Users/yansongli/Dropbox/zotero/storage/LQ5ZSWZ4/Ayoub et al. - 2020 - Model-Based Reinforcement Learning with Value-Targ.pdf;/Users/yansongli/Dropbox/zotero/storage/XX6TLGF8/2006.html}
}

@article{barrett_making_2017,
  title = {Making Friends on the Fly: {{Cooperating}} with New Teammates},
  shorttitle = {Making Friends on the Fly},
  author = {Barrett, Samuel and Rosenfeld, Avi and Kraus, Sarit and Stone, Peter},
  year = {2017},
  month = jan,
  journal = {Artificial Intelligence},
  volume = {242},
  pages = {132--171},
  issn = {00043702},
  doi = {10.1016/j.artint.2016.10.005},
  urldate = {2023-04-30},
  langid = {english},
  file = {/Users/yansongli/Dropbox/zotero/storage/9R3LU8YF/Barrett et al. - 2017 - Making friends on the fly Cooperating with new te.pdf;/Users/yansongli/Dropbox/zotero/storage/ARWP2YG4/Barrett et al. - 2017 - Making friends on the fly Cooperating with new te.pdf}
}

@article{chen_hedging_2020,
  title = {To {{Teach}} or Not to {{Teach}}? {{Decision Making Under Uncertainty}} in {{Ad Hoc Teams}}},
  author = {Stone, Peter and Kraus, Sarit},
  abstract = {In typical multiagent teamwork settings, the teammates are either programmed together, or are otherwise provided with standard communication languages and coordination protocols. In contrast, this paper presents an ad hoc team setting in which the teammates are not pre-coordinated, yet still must work together in order to achieve their common goal(s). We represent a specific instance of this scenario, in which a teammate has limited action capabilities and a fixed and known behavior, as a finite-horizon, cooperative karmed bandit. In addition to motivating and studying this novel ad hoc teamwork scenario, the paper contributes to the k-armed bandits literature by characterizing the conditions under which certain actions are potentially optimal, and by presenting a polynomial dynamic programming algorithm that solves for the optimal action when the arm payoffs come from a discrete distribution.},
  langid = {english},
  file = {/Users/yansongli/Dropbox/zotero/storage/AX8FPQMP/Stone and Kraus - To Teach or not to Teach Decision Making Under Un.pdf}
}

@article{daskalakis_near-optimal_2011,
  title = {Generating and {{Adapting}} to {{Diverse Ad-Hoc Cooperation Agents}} in {{Hanabi}}},
  author = {Canaan, Rodrigo and Gao, Xianbo and Togelius, Julian and Nealen, Andy and Menzel, Stefan},
  year = {2023},
  month = jun,
  journal = {IEEE Trans. Games},
  volume = {15},
  number = {2},
  eprint = {2004.13710},
  primaryclass = {cs},
  pages = {228--241},
  issn = {2475-1502, 2475-1510},
  doi = {10.1109/TG.2022.3169168},
  urldate = {2023-11-13},
  abstract = {Hanabi is a cooperative game that brings the problem of modeling other players to the forefront. In this game, coordinated groups of players can leverage pre-established conventions to great effect, but playing in an ad-hoc setting requires agents to adapt to its partner's strategies with no previous coordination. Evaluating an agent in this setting requires a diverse population of potential partners, but so far, the behavioral diversity of agents has not been considered in a systematic way. This paper proposes Quality Diversity algorithms as a promising class of algorithms to generate diverse populations for this purpose, and generates a population of diverse Hanabi agents using MAP-Elites. We also postulate that agents can benefit from a diverse population during training and implement a simple "meta-strategy" for adapting to an agent's perceived behavioral niche. We show this meta-strategy can work better than generalist strategies even outside the population it was trained with if its partner's behavioral niche can be correctly inferred, but in practice a partner's behavior depends and interferes with the meta-agent's own behavior, suggesting an avenue for future research in characterizing another agent's behavior during gameplay.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Neural and Evolutionary Computing},
  file = {/Users/yansongli/Dropbox/zotero/storage/9RUUYWWN/Canaan et al. - 2023 - Generating and Adapting to Diverse Ad-Hoc Cooperat.pdf;/Users/yansongli/Dropbox/zotero/storage/4JR5HQKT/2004.html}
}

@misc{daskalakis_near-optimal_2021,
  title = {{{PettingZoo}}: {{Gym}} for {{Multi-Agent Reinforcement Learning}}},
  shorttitle = {{{PettingZoo}}},
  author = {Terry, J. K. and Black, Benjamin and Grammel, Nathaniel and Jayakumar, Mario and Hari, Ananth and Sullivan, Ryan and Santos, Luis and Perez, Rodrigo and Horsch, Caroline and Dieffendahl, Clemens and Williams, Niall L. and Lokesh, Yashas and Ravi, Praveen},
  year = {2021},
  month = oct,
  number = {arXiv:2009.14471},
  eprint = {2009.14471},
  primaryclass = {cs, stat},
  publisher = {{arXiv}},
  urldate = {2023-11-06},
  abstract = {This paper introduces the PettingZoo library and the accompanying Agent Environment Cycle ("AEC") games model. PettingZoo is a library of diverse sets of multi-agent environments with a universal, elegant Python API. PettingZoo was developed with the goal of accelerating research in Multi-Agent Reinforcement Learning ("MARL"), by making work more interchangeable, accessible and reproducible akin to what OpenAI's Gym library did for single-agent reinforcement learning. PettingZoo's API, while inheriting many features of Gym, is unique amongst MARL APIs in that it's based around the novel AEC games model. We argue, in part through case studies on major problems in popular MARL environments, that the popular game models are poor conceptual models of games commonly used in MARL and accordingly can promote confusing bugs that are hard to detect, and that the AEC games model addresses these problems.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Machine Learning,Computer Science - Multiagent Systems,Statistics - Machine Learning},
  file = {/Users/yansongli/Dropbox/zotero/storage/46EBXWMW/Terry et al. - 2021 - PettingZoo Gym for Multi-Agent Reinforcement Lear.pdf;/Users/yansongli/Dropbox/zotero/storage/BFU8MHTT/Terry et al. - 2021 - PettingZoo Gym for Multi-Agent Reinforcement Lear.pdf;/Users/yansongli/Dropbox/zotero/storage/T3QY76GS/2009.html}
}

@article{dodampegama_back_2023,
  title = {Back to the {{Future}}: {{Toward}} a {{Hybrid Architecture}} for {{Ad Hoc Teamwork}}},
  shorttitle = {Back to the {{Future}}},
  author = {Dodampegama, Hasra and Sridharan, Mohan},
  year = {2023},
  month = jun,
  journal = {AAAI},
  volume = {37},
  number = {1},
  pages = {3--10},
  issn = {2374-3468, 2159-5399},
  doi = {10.1609/aaai.v37i1.25070},
  urldate = {2023-11-14},
  abstract = {State of the art methods for ad hoc teamwork, i.e., for collaboration without prior coordination, often use a long history of prior observations to model the behavior of other agents (or agent types) and to determine the ad hoc agent's behavior. In many practical domains, it is difficult to obtain large training datasets, and it is necessary to quickly revise the existing models to account for changes in team composition or the domain. Our architecture builds on the principles of stepwise refinement and ecological rationality to enable an ad hoc agent to perform non-monotonic logical reasoning with prior commonsense domain knowledge and models learned rapidly from limited examples to predict the behavior of other agents. In the simulated multiagent collaboration domain Fort Attack, we experimentally demonstrate that our architecture enables an ad hoc agent to adapt to changes in the behavior of other agents, and provides enhanced transparency and better performance than a state of the art data-driven baseline.},
  langid = {english},
  file = {/Users/yansongli/Dropbox/zotero/storage/2VZ7PE5W/Dodampegama and Sridharan - 2023 - Back to the Future Toward a Hybrid Architecture f.pdf}
}

@misc{dong_asymptotic_2023,
  title = {Asymptotic {{Instance-Optimal Algorithms}} for {{Interactive Decision Making}}},
  author = {Dong, Kefan and Ma, Tengyu},
  year = {2023},
  month = jun,
  number = {arXiv:2206.02326},
  eprint = {2206.02326},
  primaryclass = {cs},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.2206.02326},
  urldate = {2023-12-07},
  abstract = {Past research on interactive decision making problems (bandits, reinforcement learning, etc.) mostly focuses on the minimax regret that measures the algorithm's performance on the hardest instance. However, an ideal algorithm should adapt to the complexity of a particular problem instance and incur smaller regrets on easy instances than worst-case instances. In this paper, we design the first asymptotic instance-optimal algorithm for general interactive decision making problems with finite number of decisions under mild conditions. On every instance \$f\$, our algorithm outperforms all consistent algorithms (those achieving non-trivial regrets on all instances), and has asymptotic regret \${\textbackslash}mathcal\{C\}(f) {\textbackslash}ln n\$, where \${\textbackslash}mathcal\{C\}(f)\$ is an exact characterization of the complexity of \$f\$. The key step of the algorithm involves hypothesis testing with active data collection. It computes the most economical decisions with which the algorithm collects observations to test whether an estimated instance is indeed correct; thus, the complexity \${\textbackslash}mathcal\{C\}(f)\$ is the minimum cost to test the instance \$f\$ against other instances. Our results, instantiated on concrete problems, recover the classical gap-dependent bounds for multi-armed bandits [Lai and Robbins, 1985] and prior works on linear bandits [Lattimore and Szepesvari, 2017], and improve upon the previous best instance-dependent upper bound [Xu et al., 2021] for reinforcement learning.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Machine Learning},
  file = {/Users/yansongli/Dropbox/zotero/storage/KBS72HSJ/Dong and Ma - 2023 - Asymptotic Instance-Optimal Algorithms for Interac.pdf;/Users/yansongli/Dropbox/zotero/storage/G8JE88JA/2206.html}
}

@incollection{dragan_robot_2017,
  title = {Ad {{Hoc Teamwork}} in the {{Presence}} of {{Non-stationary Teammates}}},
  booktitle = {Progress in {{Artificial Intelligence}}},
  author = {Santos, Pedro M. and Ribeiro, Jo{\~a}o G. and Sardinha, Alberto and Melo, Francisco S.},
  editor = {Marreiros, Goreti and Melo, Francisco S. and Lau, Nuno and Lopes Cardoso, Henrique and Reis, Lu{\'i}s Paulo},
  year = {2021},
  volume = {12981},
  pages = {648--660},
  publisher = {{Springer International Publishing}},
  address = {{Cham}},
  doi = {10.1007/978-3-030-86230-5_51},
  urldate = {2023-11-14},
  abstract = {In this paper we address the problem of ad hoc teamwork and contribute a novel approach, PPAS, that is able to handle non-stationary teammates. Current approaches to ad hoc teamwork assume that the (potentially unknown) teammates behave in a stationary way, which is a significant limitation in real world conditions, since humans and other intelligent systems do not necessarily follow strict policies. In our work we highlight the current limitations of state-of-the-art approaches to ad hoc teamwork problem in the presence of non-stationary teammate, and propose a novel solution that alleviates the stationarity assumption by combining ad hoc teamwork with adversarial online prediction. The proposed architecture is called PLASTIC Policy with Adversarial Selection, or PPAS. We showcase the effectiveness of our approach through an empirical evaluation in the half-field offense environment. Our results show that it is possible to cooperate in an ad hoc manner with nonstationary teammates in complex environments.},
  isbn = {978-3-030-86229-9 978-3-030-86230-5},
  langid = {english},
  file = {/Users/yansongli/Dropbox/zotero/storage/STYEW8VD/Santos et al. - 2021 - Ad Hoc Teamwork in the Presence of Non-stationary .pdf}
}

@misc{hu_other-play_2021,
  title = {"{{Other-Play}}" for {{Zero-Shot Coordination}}},
  author = {Hu, Hengyuan and Lerer, Adam and Peysakhovich, Alex and Foerster, Jakob},
  year = {2021},
  month = may,
  number = {arXiv:2003.02979},
  eprint = {2003.02979},
  primaryclass = {cs},
  publisher = {{arXiv}},
  urldate = {2023-11-14},
  abstract = {We consider the problem of zero-shot coordination - constructing AI agents that can coordinate with novel partners they have not seen before (e.g. humans). Standard Multi-Agent Reinforcement Learning (MARL) methods typically focus on the self-play (SP) setting where agents construct strategies by playing the game with themselves repeatedly. Unfortunately, applying SP naively to the zero-shot coordination problem can produce agents that establish highly specialized conventions that do not carry over to novel partners they have not been trained with. We introduce a novel learning algorithm called other-play (OP), that enhances self-play by looking for more robust strategies, exploiting the presence of known symmetries in the underlying problem. We characterize OP theoretically as well as experimentally. We study the cooperative card game Hanabi and show that OP agents achieve higher scores when paired with independently trained agents. In preliminary results we also show that our OP agents obtains higher average scores when paired with human players, compared to state-of-the-art SP agents.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Artificial Intelligence},
  file = {/Users/yansongli/Dropbox/zotero/storage/TKMVS2G9/Hu et al. - 2021 - Other-Play for Zero-Shot Coordination.pdf;/Users/yansongli/Dropbox/zotero/storage/KW2UHABD/2003.html}
}

@misc{liu_learning_2022-1,
  title = {Who {{Needs}} to {{Know}}? {{Minimal Knowledge}} for {{Optimal Coordination}}},
  shorttitle = {Who {{Needs}} to {{Know}}?},
  author = {Lauffer, Niklas and Shah, Ameesh and Carroll, Micah and Dennis, Michael and Russell, Stuart},
  year = {2023},
  month = jul,
  number = {arXiv:2306.09309},
  eprint = {2306.09309},
  primaryclass = {cs},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.2306.09309},
  urldate = {2023-11-26},
  abstract = {To optimally coordinate with others in cooperative games, it is often crucial to have information about one's collaborators: successful driving requires understanding which side of the road to drive on. However, not every feature of collaborators is strategically relevant: the fine-grained acceleration of drivers may be ignored while maintaining optimal coordination. We show that there is a well-defined dichotomy between strategically relevant and irrelevant information. Moreover, we show that, in dynamic games, this dichotomy has a compact representation that can be efficiently computed via a Bellman backup operator. We apply this algorithm to analyze the strategically relevant information for tasks in both a standard and a partially observable version of the Overcooked environment. Theoretical and empirical results show that our algorithms are significantly more efficient than baselines. Videos are available at https://minknowledge.github.io.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Multiagent Systems,I.2.11,I.2.6},
  file = {/Users/yansongli/Dropbox/zotero/storage/YVIKWCR7/Lauffer et al. - 2023 - Who Needs to Know Minimal Knowledge for Optimal C.pdf;/Users/yansongli/Dropbox/zotero/storage/JGCT97CJ/2306.html}
}

@misc{liu_one_2023,
  title = {One {{Objective}} to {{Rule Them All}}: {{A Maximization Objective Fusing Estimation}} and {{Planning}} for {{Exploration}}},
  shorttitle = {One {{Objective}} to {{Rule Them All}}},
  author = {Liu, Zhihan and Lu, Miao and Xiong, Wei and Zhong, Han and Hu, Hao and Zhang, Shenao and Zheng, Sirui and Yang, Zhuoran and Wang, Zhaoran},
  year = {2023},
  month = may,
  number = {arXiv:2305.18258},
  eprint = {2305.18258},
  primaryclass = {cs, math, stat},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.2305.18258},
  urldate = {2023-06-03},
  abstract = {In online reinforcement learning (online RL), balancing exploration and exploitation is crucial for finding an optimal policy in a sample-efficient way. To achieve this, existing sample-efficient online RL algorithms typically consist of three components: estimation, planning, and exploration. However, in order to cope with general function approximators, most of them involve impractical algorithmic components to incentivize exploration, such as optimization within data-dependent level-sets or complicated sampling procedures. To address this challenge, we propose an easy-to-implement RL framework called {\textbackslash}textit\{Maximize to Explore\} ({\textbackslash}texttt\{MEX\}), which only needs to optimize {\textbackslash}emph\{unconstrainedly\} a single objective that integrates the estimation and planning components while balancing exploration and exploitation automatically. Theoretically, we prove that {\textbackslash}texttt\{MEX\} achieves a sublinear regret with general function approximations for Markov decision processes (MDP) and is further extendable to two-player zero-sum Markov games (MG). Meanwhile, we adapt deep RL baselines to design practical versions of {\textbackslash}texttt\{MEX\}, in both model-free and model-based manners, which can outperform baselines by a stable margin in various MuJoCo environments with sparse rewards. Compared with existing sample-efficient online RL algorithms with general function approximations, {\textbackslash}texttt\{MEX\} achieves similar sample efficiency while enjoying a lower computational cost and is more compatible with modern deep RL methods.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computer Science and Game Theory,Computer Science - Machine Learning,Mathematics - Optimization and Control,Statistics - Machine Learning},
  file = {/Users/yansongli/Dropbox/zotero/storage/DPZN5XT7/Liu et al. - 2023 - One Objective to Rule Them All A Maximization Obj.pdf;/Users/yansongli/Dropbox/zotero/storage/U3IZXPTD/2305.html}
}

@article{liu_stein_nodate,
  title = {Autonomous Agents Modelling Other Agents: {{A}} Comprehensive Survey and Open Problems},
  shorttitle = {Autonomous Agents Modelling Other Agents},
  author = {Albrecht, Stefano V. and Stone, Peter},
  year = {2018},
  month = may,
  journal = {Artificial Intelligence},
  volume = {258},
  pages = {66--95},
  issn = {00043702},
  doi = {10.1016/j.artint.2018.01.002},
  urldate = {2023-04-27},
  langid = {english},
  file = {/Users/yansongli/Dropbox/zotero/storage/IFH3CE54/Albrecht and Stone - 2018 - Autonomous agents modelling other agents A compre.pdf}
}

@inproceedings{lupu_trajectory_2021,
  title = {Trajectory {{Diversity}} for {{Zero-Shot Coordination}}},
  booktitle = {Proceedings of the 38th {{International Conference}} on {{Machine Learning}}},
  author = {Lupu, Andrei and Cui, Brandon and Hu, Hengyuan and Foerster, Jakob},
  year = {2021},
  month = jul,
  pages = {7204--7213},
  publisher = {{PMLR}},
  issn = {2640-3498},
  urldate = {2023-11-13},
  abstract = {We study the problem of zero-shot coordination (ZSC), where agents must independently produce strategies for a collaborative game that are compatible with novel partners not seen during training. Our first contribution is to consider the need for diversity in generating such agents. Because self-play (SP) agents control their own trajectory distribution during training, each policy typically only performs well on this exact distribution. As a result, they achieve low scores in ZSC, since playing with another agent is likely to put them in situations they have not encountered during training. To address this issue, we train a common best response (BR) to a population of agents, which we regulate to be diverse. To this end, we introduce {\textbackslash}textit\{Trajectory Diversity\} (TrajeDi) {\textendash} a differentiable objective for generating diverse reinforcement learning policies. We derive TrajeDi as a generalization of the Jensen-Shannon divergence between policies and motivate it experimentally in two simple settings. We then focus on the collaborative card game Hanabi, demonstrating the scalability of our method and improving upon the cross-play scores of both independently trained SP agents and BRs to unregularized populations.},
  langid = {english},
  file = {/Users/yansongli/Dropbox/zotero/storage/4397X63S/Lupu et al. - 2021 - Trajectory Diversity for Zero-Shot Coordination.pdf;/Users/yansongli/Dropbox/zotero/storage/EEFED7V6/Lupu et al. - 2021 - Trajectory Diversity for Zero-Shot Coordination.pdf}
}

@misc{mao_near-optimal_2021,
  title = {Defensive {{Universal Learning}} with {{Experts}}},
  author = {Poland, Jan and Hutter, Marcus},
  year = {2005},
  month = jul,
  number = {arXiv:cs/0507044},
  eprint = {cs/0507044},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.cs/0507044},
  urldate = {2023-11-20},
  abstract = {This paper shows how universal learning can be achieved with expert advice. To this aim, we specify an experts algorithm with the following characteristics: (a) it uses only feedback from the actions actually chosen (bandit setup), (b) it can be applied with countably infinite expert classes, and (c) it copes with losses that may grow in time appropriately slowly. We prove loss bounds against an adaptive adversary. From this, we obtain a master algorithm for "reactive" experts problems, which means that the master's actions may influence the behavior of the adversary. Our algorithm can significantly outperform standard experts algorithms on such problems. Finally, we combine it with a universal expert class. The resulting universal learner performs -- in a certain sense -- almost as well as any computable strategy, for any online decision problem. We also specify the (worst-case) convergence speed, which is very slow.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Machine Learning,G.3,I.2.6},
  file = {/Users/yansongli/Dropbox/zotero/storage/N52HEIBP/Poland and Hutter - 2005 - Defensive Universal Learning with Experts.pdf;/Users/yansongli/Dropbox/zotero/storage/IGE3C9HQ/0507044.html}
}

@misc{noauthor_topics_nodate,
  title = {Machine {{Theory}} of {{Mind}}},
  author = {Rabinowitz, Neil C. and Perbet, Frank and Song, H. Francis and Zhang, Chiyuan and Eslami, S. M. Ali and Botvinick, Matthew},
  year = {2018},
  month = mar,
  number = {arXiv:1802.07740},
  eprint = {1802.07740},
  primaryclass = {cs},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.1802.07740},
  urldate = {2023-04-30},
  abstract = {Theory of mind (ToM; Premack \& Woodruff, 1978) broadly refers to humans' ability to represent the mental states of others, including their desires, beliefs, and intentions. We propose to train a machine to build such models too. We design a Theory of Mind neural network -- a ToMnet -- which uses meta-learning to build models of the agents it encounters, from observations of their behaviour alone. Through this process, it acquires a strong prior model for agents' behaviour, as well as the ability to bootstrap to richer predictions about agents' characteristics and mental states using only a small number of behavioural observations. We apply the ToMnet to agents behaving in simple gridworld environments, showing that it learns to model random, algorithmic, and deep reinforcement learning agents from varied populations, and that it passes classic ToM tasks such as the "Sally-Anne" test (Wimmer \& Perner, 1983; Baron-Cohen et al., 1985) of recognising that others can hold false beliefs about the world. We argue that this system -- which autonomously learns how to model other agents in its world -- is an important step forward for developing multi-agent AI systems, for building intermediating technology for machine-human interaction, and for advancing the progress on interpretable AI.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Artificial Intelligence},
  file = {/Users/yansongli/Dropbox/zotero/storage/4VVCG3JE/Rabinowitz et al. - 2018 - Machine Theory of Mind.pdf;/Users/yansongli/Dropbox/zotero/storage/IHEE46LI/1802.html}
}

@misc{osband_model-based_2014,
  title = {Model-Based {{Reinforcement Learning}} and the {{Eluder Dimension}}},
  author = {Osband, Ian and Van Roy, Benjamin},
  year = {2014},
  month = oct,
  number = {arXiv:1406.1853},
  eprint = {1406.1853},
  primaryclass = {cs, stat},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.1406.1853},
  urldate = {2023-11-27},
  abstract = {We consider the problem of learning to optimize an unknown Markov decision process (MDP). We show that, if the MDP can be parameterized within some known function class, we can obtain regret bounds that scale with the dimensionality, rather than cardinality, of the system. We characterize this dependence explicitly as \${\textbackslash}tilde\{O\}({\textbackslash}sqrt\{d\_K d\_E T\})\$ where \$T\$ is time elapsed, \$d\_K\$ is the Kolmogorov dimension and \$d\_E\$ is the {\textbackslash}emph\{eluder dimension\}. These represent the first unified regret bounds for model-based reinforcement learning and provide state of the art guarantees in several important settings. Moreover, we present a simple and computationally efficient algorithm {\textbackslash}emph\{posterior sampling for reinforcement learning\} (PSRL) that satisfies these bounds.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/Users/yansongli/Dropbox/zotero/storage/86KIIV57/Osband and Van Roy - 2014 - Model-based Reinforcement Learning and the Eluder .pdf;/Users/yansongli/Dropbox/zotero/storage/NEAPLVAB/1406.html}
}

@article{rahman_generating_2023,
  title = {Generating {{Teammates}} for {{Training Robust Ad Hoc Teamwork Agents}} via {{Best-Response Diversity}}},
  author = {Rahman, Arrasy and Fosong, Elliot and Carlucho, Ignacio and Albrecht, Stefano V.},
  year = {2023},
  month = may,
  journal = {Transactions on Machine Learning Research},
  issn = {2835-8856},
  urldate = {2023-05-29},
  abstract = {Ad hoc teamwork (AHT) is the challenge of designing a robust learner agent that effectively collaborates with unknown teammates without prior coordination mechanisms. Early approaches address the AHT challenge by training the learner with a diverse set of handcrafted teammate policies, usually designed based on an expert's domain knowledge about the policies the learner may encounter. However, implementing teammate policies for training based on domain knowledge is not always feasible. In such cases, recent approaches attempted to improve the robustness of the learner by training it with teammate policies generated by optimising information-theoretic diversity metrics. The problem with optimising existing information-theoretic diversity metrics for teammate policy generation is the emergence of superficially different teammates. When used for AHT training, superficially different teammate behaviours may not improve a learner's robustness during collaboration with unknown teammates. In this paper, we present an automated teammate policy generation method optimising the Best-Response Diversity (BRDiv) metric, which measures diversity based on the compatibility of teammate policies in terms of returns. We evaluate our approach in environments with multiple valid coordination strategies, comparing against methods optimising information-theoretic diversity metrics and an ablation not optimising any diversity metric. Our experiments indicate that optimising BRDiv yields a diverse set of training teammate policies that improve the learner's performance relative to previous teammate generation approaches when collaborating with near-optimal previously unseen teammate policies.},
  langid = {english},
  file = {/Users/yansongli/Dropbox/zotero/storage/HIF57VY8/Rahman et al. - 2023 - Generating Teammates for Training Robust Ad Hoc Te.pdf}
}

@inproceedings{rakhlin_optimization_2013,
  title = {On the {{Impossibility}} of {{Learning}} to {{Cooperate}} with {{Adaptive Partner Strategies}} in {{Repeated Games}}},
  booktitle = {Proceedings of the 39th {{International Conference}} on {{Machine Learning}}},
  author = {Loftin, Robert and Oliehoek, Frans A.},
  year = {2022},
  month = jun,
  pages = {14197--14209},
  publisher = {{PMLR}},
  issn = {2640-3498},
  urldate = {2023-11-03},
  abstract = {Learning to cooperate with other agents is challenging when those agents also possess the ability to adapt to our own behavior. Practical and theoretical approaches to learning in cooperative settings typically assume that other agents' behaviors are stationary, or else make very specific assumptions about other agents' learning processes. The goal of this work is to understand whether we can reliably learn to cooperate with other agents without such restrictive assumptions, which are unlikely to hold in real-world applications. Our main contribution is a set of impossibility results, which show that no learning algorithm can reliably learn to cooperate with all possible adaptive partners in a repeated matrix game, even if that partner is guaranteed to cooperate with some stationary strategy. Motivated by these results, we then discuss potential alternative assumptions which capture the idea that an adaptive partner will only adapt rationally to our behavior.},
  langid = {english},
  file = {/Users/yansongli/Dropbox/zotero/storage/HXCX6W5C/Loftin and Oliehoek - 2022 - On the Impossibility of Learning to Cooperate with.pdf}
}

@article{rantzer_minimax_2021,
  title = {Ad Hoc Teamwork by Learning Teammates' Task},
  author = {Melo, Francisco S. and Sardinha, Alberto},
  year = {2016},
  month = mar,
  journal = {Auton Agent Multi-Agent Syst},
  volume = {30},
  number = {2},
  pages = {175--219},
  issn = {1387-2532, 1573-7454},
  doi = {10.1007/s10458-015-9280-x},
  urldate = {2023-11-13},
  abstract = {This paper addresses the problem of ad hoc teamwork, where a learning agent engages in a cooperative task with other (unknown) agents. The agent must effectively coordinate with the other agents towards completion of the intended task, not relying on any pre-defined coordination strategy. We contribute a new perspective on the ad hoc teamwork problem and propose that, in general, the learning agent should not only identify (and coordinate with) the teammates' strategy but also identify the task to be completed. In our approach to the ad hoc teamwork problem, we represent tasks as fully cooperative matrix games. Relying exclusively on observations of the behavior of the teammates, the learning agent must identify the task at hand (namely, the corresponding payoff function) from a set of possible tasks and adapt to the teammates' behavior. Teammates are assumed to follow a boundedrationality best-response model and thus also adapt their behavior to that of the learning agent. We formalize the ad hoc teamwork problem as a sequential decision problem and propose two novel approaches to address it. In particular, we propose (i) the use of an online learning approach that considers the different tasks depending on their ability to predict the behavior of the teammate; and (ii) a decision-theoretic approach that models the ad hoc teamwork problem as a partially observable Markov decision problem. We provide theoretical bounds of the performance of both approaches and evaluate their performance in several domains of different complexity.},
  langid = {english},
  file = {/Users/yansongli/Dropbox/zotero/storage/VVCFUNWF/Melo and Sardinha - 2016 - Ad hoc teamwork by learning teammates’ task.pdf}
}

@misc{sarkar_diverse_2023,
  title = {Diverse {{Conventions}} for {{Human-AI Collaboration}}},
  author = {Sarkar, Bidipta and Shih, Andy and Sadigh, Dorsa},
  year = {2023},
  month = oct,
  number = {arXiv:2310.15414},
  eprint = {2310.15414},
  primaryclass = {cs},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.2310.15414},
  urldate = {2023-10-30},
  abstract = {Conventions are crucial for strong performance in cooperative multi-agent games, because they allow players to coordinate on a shared strategy without explicit communication. Unfortunately, standard multi-agent reinforcement learning techniques, such as self-play, converge to conventions that are arbitrary and non-diverse, leading to poor generalization when interacting with new partners. In this work, we present a technique for generating diverse conventions by (1) maximizing their rewards during self-play, while (2) minimizing their rewards when playing with previously discovered conventions (cross-play), stimulating conventions to be semantically different. To ensure that learned policies act in good faith despite the adversarial optimization of cross-play, we introduce {\textbackslash}emph\{mixed-play\}, where an initial state is randomly generated by sampling self-play and cross-play transitions and the player learns to maximize the self-play reward from this initial state. We analyze the benefits of our technique on various multi-agent collaborative games, including Overcooked, and find that our technique can adapt to the conventions of humans, surpassing human-level performance when paired with real users.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning,Computer Science - Multiagent Systems},
  file = {/Users/yansongli/Dropbox/zotero/storage/3AARC6GD/Sarkar et al. - 2023 - Diverse Conventions for Human-AI Collaboration.pdf;/Users/yansongli/Dropbox/zotero/storage/3J9NBN88/2310.html}
}

@article{simonetto_time-varying_2020,
  title = {Quantifying {{Human Rationality}} in {{Ad-hoc Teamwork}}},
  author = {Hanina, Yair and Mirsky, Reuth and Macke, William and Stone, Peter},
  abstract = {Ad-hoc teamwork is defined as the task of collaborating with teammates without pre-coordination. When the ad hoc agent is a robot that needs to collaborate with people, it cannot assume that its teammates will behave optimally or legibly. By providing a means to learn human policies in ad-hoc teamwork, this work will help create robots that can adapt to a new human agent and work together to achieve a common goal. We focus on a simple, yet powerful model for representing agents using the concept of bounded rationality. Our preliminary results exemplify how such a model can be used in a domain from the ad-hoc teamwork literature called ``the tool fetching domain''.},
  langid = {english},
  file = {/Users/yansongli/Dropbox/zotero/storage/G7JHWWKS/Hanina et al. - Quantifying Human Rationality in Ad-hoc Teamwork.pdf}
}

@article{stone_ad_2010,
  title = {Ad {{Hoc Autonomous Agent Teams}}: {{Collaboration}} without {{Pre-Coordination}}},
  shorttitle = {Ad {{Hoc Autonomous Agent Teams}}},
  author = {Stone, Peter and Kaminka, Gal and Kraus, Sarit and Rosenschein, Jeffrey},
  year = {2010},
  month = jul,
  journal = {AAAI},
  volume = {24},
  number = {1},
  pages = {1504--1509},
  issn = {2374-3468, 2159-5399},
  doi = {10.1609/aaai.v24i1.7529},
  urldate = {2023-09-07},
  abstract = {As autonomous agents proliferate in the real world, both in software and robotic settings, they will increasingly need to band together for cooperative activities with previously unfamiliar teammates. In such ad hoc team settings, team strategies cannot be developed a priori. Rather, an agent must be prepared to cooperate with many types of teammates: it must collaborate without pre-coordination. This paper challenges the AI community to develop theory and to implement prototypes of ad hoc team agents. It defines the concept of ad hoc team agents, specifies an evaluation paradigm, and provides examples of possible theoretical and empirical approaches to challenge. The goal is to encourage progress towards this ambitious, newly realistic, and increasingly important research goal.},
  langid = {english},
  file = {/Users/yansongli/Dropbox/zotero/storage/NVVTM8C3/Stone et al. - 2010 - Ad Hoc Autonomous Agent Teams Collaboration witho.pdf}
}

@misc{sundararajan_analysis_2020,
  title = {Online {{Bandit Learning}} against an {{Adaptive Adversary}}: From {{Regret}} to {{Policy Regret}}},
  shorttitle = {Online {{Bandit Learning}} against an {{Adaptive Adversary}}},
  author = {Arora, Raman and Dekel, Ofer},
  year = {2012},
  month = jun,
  number = {arXiv:1206.6400},
  eprint = {1206.6400},
  primaryclass = {cs, stat},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.1206.6400},
  urldate = {2023-02-28},
  abstract = {Online learning algorithms are designed to learn even when their input is generated by an adversary. The widely-accepted formal definition of an online algorithm's ability to learn is the game-theoretic notion of regret. We argue that the standard definition of regret becomes inadequate if the adversary is allowed to adapt to the online algorithm's actions. We define the alternative notion of policy regret, which attempts to provide a more meaningful way to measure an online algorithm's performance against adaptive adversaries. Focusing on the online bandit setting, we show that no bandit algorithm can guarantee a sublinear policy regret against an adaptive adversary with unbounded memory. On the other hand, if the adversary's memory is bounded, we present a general technique that converts any bandit algorithm with a sublinear regret bound into an algorithm with a sublinear policy regret bound. We extend this result to other variants of regret, such as switching regret, internal regret, and swap regret.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/Users/yansongli/Dropbox/zotero/storage/S3IVMJDB/Arora et al. - 2012 - Online Bandit Learning against an Adaptive Adversa.pdf;/Users/yansongli/Dropbox/zotero/storage/NSL5F483/1206.html}
}

@article{sutton_between_1999,
  title = {Between {{MDPs}} and Semi-{{MDPs}}: {{A}} Framework for Temporal Abstraction in Reinforcement Learning},
  shorttitle = {Between {{MDPs}} and Semi-{{MDPs}}},
  author = {Sutton, Richard S. and Precup, Doina and Singh, Satinder},
  year = {1999},
  month = aug,
  journal = {Artificial Intelligence},
  volume = {112},
  number = {1-2},
  pages = {181--211},
  issn = {00043702},
  doi = {10.1016/S0004-3702(99)00052-1},
  urldate = {2023-06-27},
  abstract = {Learning, planning, and representing knowledge at multiple levels of temporal abstraction are key, longstanding challenges for AI. In this paper we consider how these challenges can be addressed within the mathematical framework of reinforcement learning and Markov decision processes (MDPs). We extend the usual notion of action in this framework to include options{\textemdash}closed-loop policies for taking action over a period of time. Examples of options include picking up an object, going to lunch, and traveling to a distant city, as well as primitive actions such as muscle twitches and joint torques. Overall, we show that options enable temporally abstract knowledge and action to be included in the reinforcement learning framework in a natural and general way. In particular, we show that options may be used interchangeably with primitive actions in planning methods such as dynamic programming and in learning methods such as Q-learning. Formally, a set of options defined over an MDP constitutes a semi-Markov decision process (SMDP), and the theory of SMDPs provides the foundation for the theory of options. However, the most interesting issues concern the interplay between the underlying MDP and the SMDP and are thus beyond SMDP theory. We present results for three such cases: (1) we show that the results of planning with options can be used during execution to interrupt options and thereby perform even better than planned, (2) we introduce new intra-option methods that are able to learn about an option from fragments of its execution, and (3) we propose a notion of subgoal that can be used to improve the options themselves. All of these results have precursors in the existing literature; the contribution of this paper is to establish them in a simpler and more general setting with fewer changes to the existing reinforcement learning framework. In particular, we show that these results can be obtained without committing to (or ruling out) any particular approach to state abstraction, hierarchy, function approximation, or the macroutility problem. {\textcopyright} 1999 Published by Elsevier Science B.V. All rights reserved.},
  langid = {english},
  file = {/Users/yansongli/Dropbox/zotero/storage/QJPW3FLZ/Sutton et al. - 1999 - Between MDPs and semi-MDPs A framework for tempor.pdf}
}

@misc{tian_safety_2021,
  title = {Scalable {{Evaluation}} of {{Multi-Agent Reinforcement Learning}} with {{Melting Pot}}},
  author = {Leibo, Joel Z. and {Du{\'e}{\~n}ez-Guzm{\'a}n}, Edgar and Vezhnevets, Alexander Sasha and Agapiou, John P. and Sunehag, Peter and Koster, Raphael and Matyas, Jayd and Beattie, Charles and Mordatch, Igor and Graepel, Thore},
  year = {2021},
  month = jul,
  number = {arXiv:2107.06857},
  eprint = {2107.06857},
  primaryclass = {cs},
  publisher = {{arXiv}},
  urldate = {2023-11-13},
  abstract = {Existing evaluation suites for multi-agent reinforcement learning (MARL) do not assess generalization to novel situations as their primary objective (unlike supervised-learning benchmarks). Our contribution, Melting Pot, is a MARL evaluation suite that fills this gap, and uses reinforcement learning to reduce the human labor required to create novel test scenarios. This works because one agent's behavior constitutes (part of) another agent's environment. To demonstrate scalability, we have created over 80 unique test scenarios covering a broad range of research topics such as social dilemmas, reciprocity, resource sharing, and task partitioning. We apply these test scenarios to standard MARL training algorithms, and demonstrate how Melting Pot reveals weaknesses not apparent from training performance alone.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Multiagent Systems},
  file = {/Users/yansongli/Dropbox/zotero/storage/C7YJEW9L/Leibo et al. - 2021 - Scalable Evaluation of Multi-Agent Reinforcement L.pdf;/Users/yansongli/Dropbox/zotero/storage/RCLS236W/2107.html}
}

@incollection{wagener_online_2019,
  title = {A {{Survey}} of {{Ad Hoc Teamwork Research}}},
  booktitle = {Multi-{{Agent Systems}}},
  author = {Mirsky, Reuth and Carlucho, Ignacio and Rahman, Arrasy and Fosong, Elliot and Macke, William and Sridharan, Mohan and Stone, Peter and Albrecht, Stefano V.},
  editor = {Baumeister, Dorothea and Rothe, J{\"o}rg},
  year = {2022},
  volume = {13442},
  pages = {275--293},
  publisher = {{Springer International Publishing}},
  address = {{Cham}},
  doi = {10.1007/978-3-031-20614-6_16},
  urldate = {2023-04-27},
  abstract = {Ad hoc teamwork is the research problem of designing agents that can collaborate with new teammates without prior coordination. This survey makes a two-fold contribution: First, it provides a structured description of the different facets of the ad hoc teamwork problem. Second, it discusses the progress that has been made in the field so far, and identifies the immediate and long-term open problems that need to be addressed in ad hoc teamwork.},
  isbn = {978-3-031-20613-9 978-3-031-20614-6},
  langid = {english},
  file = {/Users/yansongli/Dropbox/zotero/storage/9YM8QPDM/Mirsky et al. - 2022 - A Survey of Ad Hoc Teamwork Research.pdf}
}

@misc{wagenmaker_instance-optimality_2023,
  title = {Instance-{{Optimality}} in {{Interactive Decision Making}}: {{Toward}} a {{Non-Asymptotic Theory}}},
  shorttitle = {Instance-{{Optimality}} in {{Interactive Decision Making}}},
  author = {Wagenmaker, Andrew and Foster, Dylan J.},
  year = {2023},
  month = apr,
  number = {arXiv:2304.12466},
  eprint = {2304.12466},
  primaryclass = {cs, stat},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.2304.12466},
  urldate = {2023-12-07},
  abstract = {We consider the development of adaptive, instance-dependent algorithms for interactive decision making (bandits, reinforcement learning, and beyond) that, rather than only performing well in the worst case, adapt to favorable properties of real-world instances for improved performance. We aim for instance-optimality, a strong notion of adaptivity which asserts that, on any particular problem instance, the algorithm under consideration outperforms all consistent algorithms. Instance-optimality enjoys a rich asymptotic theory originating from the work of {\textbackslash}citet\{lai1985asymptotically,graves1997asymptotically\}, but non-asymptotic guarantees have remained elusive outside of certain special cases. Even for problems as simple as tabular reinforcement learning, existing algorithms do not attain instance-optimal performance until the number of rounds of interaction is doubly exponential in the number of states. In this paper, we take the first step toward developing a non-asymptotic theory of instance-optimal decision making with general function approximation. We introduce a new complexity measure, the Allocation-Estimation Coefficient (AEC), and provide a new algorithm, \${\textbackslash}mathsf\{AE\}\^2\$, which attains non-asymptotic instance-optimal performance at a rate controlled by the AEC. Our results recover the best known guarantees for well-studied problems such as finite-armed and linear bandits and, when specialized to tabular reinforcement learning, attain the first instance-optimal regret bounds with polynomial dependence on all problem parameters, improving over prior work exponentially. We complement these results with lower bounds that show that i) existing notions of statistical complexity are insufficient to derive non-asymptotic guarantees, and ii) under certain technical conditions, boundedness of the AEC is necessary to learn an instance-optimal allocation of decisions in finite time.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/Users/yansongli/Dropbox/zotero/storage/IE4SN4KS/Wagenmaker and Foster - 2023 - Instance-Optimality in Interactive Decision Making.pdf;/Users/yansongli/Dropbox/zotero/storage/G74Z94U6/2304.html}
}

@misc{xie_learning_2020,
  title = {Learning {{Latent Representations}} to {{Influence Multi-Agent Interaction}}},
  author = {Xie, Annie and Losey, Dylan P. and Tolsma, Ryan and Finn, Chelsea and Sadigh, Dorsa},
  year = {2020},
  month = nov,
  number = {arXiv:2011.06619},
  eprint = {2011.06619},
  primaryclass = {cs},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.2011.06619},
  urldate = {2023-04-30},
  abstract = {Seamlessly interacting with humans or robots is hard because these agents are non-stationary. They update their policy in response to the ego agent's behavior, and the ego agent must anticipate these changes to co-adapt. Inspired by humans, we recognize that robots do not need to explicitly model every low-level action another agent will make; instead, we can capture the latent strategy of other agents through high-level representations. We propose a reinforcement learning-based framework for learning latent representations of an agent's policy, where the ego agent identifies the relationship between its behavior and the other agent's future strategy. The ego agent then leverages these latent dynamics to influence the other agent, purposely guiding them towards policies suitable for co-adaptation. Across several simulated domains and a real-world air hockey game, our approach outperforms the alternatives and learns to influence the other agent.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning,Computer Science - Robotics},
  file = {/Users/yansongli/Dropbox/zotero/storage/3B4G55AN/Xie et al. - 2020 - Learning Latent Representations to Influence Multi.pdf;/Users/yansongli/Dropbox/zotero/storage/87Y2XMQI/2011.html}
}

@inproceedings{yurii_introductory_nodate,
  title = {Ad {{Hoc Teamwork With Behavior Switching Agents}}},
  booktitle = {Proceedings of the {{Twenty-Eighth International Joint Conference}} on {{Artificial Intelligence}}},
  author = {Ravula, Manish and Alkoby, Shani and Stone, Peter},
  year = {2019},
  month = aug,
  pages = {550--556},
  publisher = {{International Joint Conferences on Artificial Intelligence Organization}},
  address = {{Macao, China}},
  doi = {10.24963/ijcai.2019/78},
  urldate = {2023-04-30},
  abstract = {As autonomous AI agents proliferate in the real world, they will increasingly need to cooperate with each other to achieve complex goals without always being able to coordinate in advance. This kind of cooperation, in which agents have to learn to cooperate on the fly, is called ad hoc teamwork. Many previous works investigating this setting assumed that teammates behave according to one of many predefined types that is fixed throughout the task. This assumption of stationarity in behaviors, is a strong assumption which cannot be guaranteed in many real-world settings. In this work, we relax this assumption and investigate settings in which teammates can change their types during the course of the task. This adds complexity to the planning problem as now an agent needs to recognize that a change has occurred in addition to figuring out what is the new type of the teammate it is interacting with. In this paper, we present a novel Convolutional-Neural-Networkbased Change Point Detection (CPD) algorithm for ad hoc teamwork. When evaluating our algorithm on the modified predator prey domain, we find that it outperforms existing Bayesian CPD algorithms.},
  isbn = {978-0-9992411-4-1},
  langid = {english},
  file = {/Users/yansongli/Dropbox/zotero/storage/HFKYLKYM/Ravula et al. - 2019 - Ad Hoc Teamwork With Behavior Switching Agents.pdf}
}

@misc{zhong_gec_2023,
  title = {{{GEC}}: {{A Unified Framework}} for {{Interactive Decision Making}} in {{MDP}}, {{POMDP}}, and {{Beyond}}},
  shorttitle = {{{GEC}}},
  author = {Zhong, Han and Xiong, Wei and Zheng, Sirui and Wang, Liwei and Wang, Zhaoran and Yang, Zhuoran and Zhang, Tong},
  year = {2023},
  month = jun,
  number = {arXiv:2211.01962},
  eprint = {2211.01962},
  primaryclass = {cs, math, stat},
  publisher = {{arXiv}},
  urldate = {2023-07-18},
  abstract = {We study sample efficient reinforcement learning (RL) under the general framework of interactive decision making, which includes Markov decision process (MDP), partially observable Markov decision process (POMDP), and predictive state representation (PSR) as special cases. Toward finding the minimum assumption that empowers sample efficient learning, we propose a novel complexity measure, generalized eluder coefficient (GEC), which characterizes the fundamental tradeoff between exploration and exploitation in online interactive decision making. In specific, GEC captures the hardness of exploration by comparing the error of predicting the performance of the updated policy with the in-sample training error evaluated on the historical data. We show that RL problems with low GEC form a remarkably rich class, which subsumes low Bellman eluder dimension problems, bilinear class, low witness rank problems, PO-bilinear class, and generalized regular PSR, where generalized regular PSR, a new tractable PSR class identified by us, includes nearly all known tractable POMDPs and PSRs. Furthermore, in terms of algorithm design, we propose a generic posterior sampling algorithm, which can be implemented in both model-free and model-based fashion, under both fully observable and partially observable settings. The proposed algorithm modifies the standard posterior sampling algorithm in two aspects: (i) we use an optimistic prior distribution that biases towards hypotheses with higher values and (ii) a loglikelihood function is set to be the empirical loss evaluated on the historical data, where the choice of loss function supports both model-free and model-based learning. We prove that the proposed algorithm is sample efficient by establishing a sublinear regret upper bound in terms of GEC. In summary, we provide a new and unified understanding of both fully observable and partially observable RL.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning,Mathematics - Optimization and Control,Statistics - Machine Learning},
  file = {/Users/yansongli/Dropbox/zotero/storage/YAA92JK7/Zhong et al. - 2023 - GEC A Unified Framework for Interactive Decision .pdf;/Users/yansongli/Dropbox/zotero/storage/VU9PY7T9/2211.html}
}

@misc{zintgraf_deep_2022,
  title = {Deep {{Interactive Bayesian Reinforcement Learning}} via {{Meta-Learning}}},
  author = {Zintgraf, Luisa and Devlin, Sam and Ciosek, Kamil and Whiteson, Shimon and Hofmann, Katja},
  year = {2022},
  month = apr,
  number = {arXiv:2101.03864},
  eprint = {2101.03864},
  primaryclass = {cs},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.2101.03864},
  urldate = {2023-04-30},
  abstract = {Agents that interact with other agents often do not know a priori what the other agents' strategies are, but have to maximise their own online return while interacting with and learning about others. The optimal adaptive behaviour under uncertainty over the other agents' strategies w.r.t. some prior can in principle be computed using the Interactive Bayesian Reinforcement Learning framework. Unfortunately, doing so is intractable in most settings, and existing approximation methods are restricted to small tasks. To overcome this, we propose to meta-learn approximate belief inference and Bayes-optimal behaviour for a given prior. To model beliefs over other agents, we combine sequential and hierarchical Variational Auto-Encoders, and meta-train this inference model alongside the policy. We show empirically that our approach outperforms existing methods that use a model-free approach, sample from the approximate posterior, maintain memory-free models of others, or do not fully utilise the known structure of the environment.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Machine Learning,Computer Science - Multiagent Systems},
  file = {/Users/yansongli/Dropbox/zotero/storage/DGEJKVC8/Zintgraf et al. - 2022 - Deep Interactive Bayesian Reinforcement Learning v.pdf;/Users/yansongli/Dropbox/zotero/storage/HDI8WMDQ/2101.html}
}
