
@inproceedings{abbeel_application_2007,
  title = {An {{Application}} of {{Reinforcement Learning}} to {{Aerobatic Helicopter Flight}}},
  booktitle = {Advances in {{Neural Information Processing Systems}}},
  author = {Abbeel, Pieter and Coates, Adam and Quigley, Morgan and Ng, Andrew},
  year = {2007},
  volume = {19},
  publisher = {{MIT Press}},
  file = {C\:\\Users\\Haggi\\Zotero\\storage\\6SI2HA4U\\Abbeel et al. - 2007 - An Application of Reinforcement Learning to Aeroba.pdf}
}

@inproceedings{abbeel_using_2006,
  title = {Using Inaccurate Models in Reinforcement Learning},
  booktitle = {Proceedings of the 23rd International Conference on {{Machine}} Learning  - {{ICML}} '06},
  author = {Abbeel, Pieter and Quigley, Morgan and Ng, Andrew Y.},
  year = {2006},
  pages = {1--8},
  publisher = {{ACM Press}},
  address = {{Pittsburgh, Pennsylvania}},
  doi = {10.1145/1143844.1143845},
  abstract = {In the model-based policy search approach to reinforcement learning (RL), policies are found using a model (or ``simulator'') of the Markov decision process. However, for highdimensional continuous-state tasks, it can be extremely difficult to build an accurate model, and thus often the algorithm returns a policy that works in simulation but not in real-life. The other extreme, model-free RL, tends to require infeasibly large numbers of real-life trials. In this paper, we present a hybrid algorithm that requires only an approximate model, and only a small number of real-life trials. The key idea is to successively ``ground'' the policy evaluations using real-life trials, but to rely on the approximate model to suggest local changes. Our theoretical results show that this algorithm achieves near-optimal performance in the real system, even when the model is only approximate. Empirical results also demonstrate that\textemdash when given only a crude model and a small number of real-life trials\textemdash our algorithm can obtain near-optimal performance in the real system.},
  isbn = {978-1-59593-383-6},
  langid = {english},
  file = {C\:\\Users\\Haggi\\Zotero\\storage\\4RP5LFRJ\\Abbeel et al. - 2006 - Using inaccurate models in reinforcement learning.pdf}
}

@inproceedings{atkeson_comparison_1997,
  title = {A Comparison of Direct and Model-Based Reinforcement Learning},
  booktitle = {Proceedings of {{International Conference}} on {{Robotics}} and {{Automation}}},
  author = {Atkeson, C.G. and Santamaria, J.C.},
  year = {1997},
  month = apr,
  volume = {4},
  pages = {3557-3564 vol.4},
  doi = {10.1109/ROBOT.1997.606886},
  abstract = {This paper compares direct reinforcement learning (no explicit model) and model-based reinforcement learning on a simple task: pendulum swing up. We find that in this task model-based approaches support reinforcement learning from smaller amounts of training data and efficient handling of changing goals.},
  keywords = {Computational modeling,Control system synthesis,Control systems,Educational institutions,Force control,Jacobian matrices,Learning,Robots,State-space methods,Training data},
  file = {C\:\\Users\\Haggi\\Zotero\\storage\\6P3HM33V\\Atkeson and Santamaria - 1997 - A comparison of direct and model-based reinforceme.pdf;C\:\\Users\\Haggi\\Zotero\\storage\\K5RCK6XD\\606886.html}
}

@article{bauschke_projection_1996,
  title = {On {{Projection Algorithms}} for {{Solving Convex Feasibility Problems}}},
  author = {Bauschke, Heinz H. and Borwein, Jonathan M.},
  year = {1996},
  month = sep,
  journal = {SIAM Review},
  volume = {38},
  number = {3},
  pages = {367--426},
  publisher = {{Society for Industrial and Applied Mathematics}},
  issn = {0036-1445},
  doi = {10.1137/S0036144593251710},
  abstract = {Due to their extraordinary utility and broad applicability in many areas of classical mathematics and modern physical sciences (most notably, computerized tomography), algorithms for solving convex feasibility problems continue to receive great attention. To unify, generalize, and review some of these algorithms, a very broad and flexible framework is investigated. Several crucial new concepts which allow a systematic discussion of questions on behaviour in general Hilbert spaces and on the quality of convergence are brought out. Numerous examples are given.},
  keywords = {47H09,49M45,65-02,65J05,90C25,angle between two subspaces,averaged mapping,Cimmino’s method,computerized tomography,convex feasibility problem,convex function,convex inequalities,convex programming,convex set,Fejer monotone sequence,firmly nonexpansive mapping,Hilbert space,image recovery,iterative method,Kaczmarz’s method,linear convergence,linear feasibility problem,linear inequalities,nonexpansive mapping,orthogonal projection,projection algorithm,projection method,Slater point,subdifferential,subgradient,subgradient algorithm,successive projections},
  file = {C\:\\Users\\Haggi\\Zotero\\storage\\CFAUEML7\\Bauschke and Borwein - 1996 - On Projection Algorithms for Solving Convex Feasib.pdf}
}

@article{beck_fast_2009,
  title = {A {{Fast Iterative Shrinkage-Thresholding Algorithm}} for {{Linear Inverse Problems}}},
  author = {Beck, Amir and Teboulle, Marc},
  year = {2009},
  month = jan,
  journal = {SIAM Journal on Imaging Sciences},
  volume = {2},
  number = {1},
  pages = {183--202},
  issn = {1936-4954},
  doi = {10.1137/080716542},
  abstract = {We consider the class of iterative shrinkage-thresholding algorithms (ISTA) for solving linear inverse problems arising in signal/image processing. This class of methods, which can be viewed as an extension of the classical gradient algorithm, is attractive due to its simplicity and thus is adequate for solving large-scale problems even with dense matrix data. However, such methods are also known to converge quite slowly. In this paper we present a new fast iterative shrinkage-thresholding algorithm (FISTA) which preserves the computational simplicity of ISTA but with a global rate of convergence which is proven to be significantly better, both theoretically and practically. Initial promising numerical results for wavelet-based image deblurring demonstrate the capabilities of FISTA which is shown to be faster than ISTA by several orders of magnitude.},
  langid = {english},
  file = {C\:\\Users\\Haggi\\Zotero\\storage\\KU9MM9Y3\\Beck and Teboulle - 2009 - A Fast Iterative Shrinkage-Thresholding Algorithm .pdf}
}

@book{boyd_convex_2004,
  title = {Convex Optimization},
  author = {Boyd, Stephen P. and Vandenberghe, Lieven},
  year = {2004},
  publisher = {{Cambridge University Press}},
  address = {{Cambridge, UK ; New York}},
  isbn = {978-0-521-83378-3},
  langid = {english},
  lccn = {QA402.5 .B69 2004},
  keywords = {Convex functions,Mathematical optimization},
  file = {C\:\\Users\\Haggi\\Zotero\\storage\\EDSRSB6A\\Boyd and Vandenberghe - 2004 - Convex optimization.pdf}
}

@article{bu_lqr_2019,
  title = {{{LQR}} through the {{Lens}} of {{First Order Methods}}: {{Discrete-time Case}}},
  shorttitle = {{{LQR}} through the {{Lens}} of {{First Order Methods}}},
  author = {Bu, Jingjing and Mesbahi, Afshin and Fazel, Maryam and Mesbahi, Mehran},
  year = {2019},
  month = jul,
  journal = {arXiv:1907.08921 [cs, eess, math]},
  eprint = {1907.08921},
  eprinttype = {arxiv},
  primaryclass = {cs, eess, math},
  abstract = {We consider the Linear-Quadratic-Regulator (LQR) problem in terms of optimizing a real-valued matrix function over the set of feedback gains. Such a setup facilitates examining the implications of a natural initial-state independent formulation of LQR in designing first order algorithms. It is shown that this cost function is smooth and coercive, and provide an alternate means of noting its gradient dominated property. In the process, we provide a number of analytic observations on the LQR cost when directly analyzed in terms of the feedback gain. We then examine three types of well-posed flows for LQR: gradient flow, natural gradient flow and the quasi-Newton flow. The coercive property suggests that these flows admit unique solutions while gradient dominated property indicates that the corresponding Lyapunov functionals decay at an exponential rate; we also prove that these flows are exponentially stable in the sense of Lyapunov. We then discuss the forward Euler discretization of these flows, realized as gradient descent, natural gradient descent and the quasi-Newton iteration. We present stepsize criteria for gradient descent and natural gradient descent, guaranteeing that both algorithms converge linearly to the global optima. An optimal stepsize for the quasi-Newton iteration is also proposed, guaranteeing a \$Q\$-quadratic convergence rate--and in the meantime--recovering the Hewer algorithm.},
  archiveprefix = {arXiv},
  keywords = {Electrical Engineering and Systems Science - Systems and Control,Mathematics - Optimization and Control},
  file = {C\:\\Users\\Haggi\\Zotero\\storage\\H7T96HVJ\\Bu et al. - 2019 - LQR through the Lens of First Order Methods Discr.pdf}
}

@inproceedings{chebotar_combining_2017-1,
  title = {Combining {{Model-Based}} and {{Model-Free Updates}} for {{Trajectory-Centric Reinforcement Learning}}},
  booktitle = {Proceedings of the 34th {{International Conference}} on {{Machine Learning}}},
  author = {Chebotar, Yevgen and Hausman, Karol and Zhang, Marvin and Sukhatme, Gaurav and Schaal, Stefan and Levine, Sergey},
  year = {2017},
  month = jul,
  pages = {703--711},
  publisher = {{PMLR}},
  issn = {2640-3498},
  abstract = {Reinforcement learning algorithms for real-world robotic applications must be able to handle complex, unknown dynamical systems while maintaining data-efficient learning. These requirements are handled well by model-free and model-based RL approaches, respectively. In this work, we aim to combine the advantages of these approaches. By focusing on time-varying linear-Gaussian policies, we enable a model-based algorithm based on the linear-quadratic regulator that can be integrated into the model-free framework of path integral policy improvement. We can further combine our method with guided policy search to train arbitrary parameterized policies such as deep neural networks. Our simulation and real-world experiments demonstrate that this method can solve challenging manipulation tasks with comparable or better performance than model-free methods while maintaining the sample efficiency of model-based methods.},
  langid = {english},
  file = {C\:\\Users\\Haggi\\Zotero\\storage\\6IRANM94\\Chebotar et al. - 2017 - Combining Model-Based and Model-Free Updates for T.pdf;C\:\\Users\\Haggi\\Zotero\\storage\\TG5BZ6AV\\Chebotar et al. - 2017 - Combining Model-Based and Model-Free Updates for T.pdf}
}

@book{conn_introduction_2009,
  title = {Introduction to {{Derivative-Free Optimization}}},
  author = {Conn, Andrew R. and Scheinberg, Katya and Vicente, Luis N.},
  year = {2009},
  month = jan,
  series = {{{MOS-SIAM Series}} on {{Optimization}}},
  publisher = {{Society for Industrial and Applied Mathematics}},
  doi = {10.1137/1.9780898718768},
  abstract = {For many years all three of us have been interested in, and have tried to make contributions to, derivative-free optimization. Our motivation for writing this book resulted from various circumstances. We had the opportunity to work closely with the leading contributors to the field, including especially John Dennis, Michael Powell, Philippe Toint, and Virginia Torczon. We had some knowledge of various facets of the recent developments in the area, and yet felt there was a need for a unified view, and we hoped thereby to gain a better understanding of the field. In addition we were enticed by the growing number of applications. We also felt very strongly that there was a considerable need for a textbook on derivative-free optimization, especially since the foundations, algorithms, and applications have become significantly enriched in the past decade. Finally, although the subject depends upon much that is true for, and was developed for, optimization with derivatives, the issues that arise are new. The absence of computable derivatives naturally prohibits the use of Taylor models\textemdash so common in derivative-based optimization. The fact that typical applications involve expensive function evaluations shifts the emphasis from the cost of the linear algebra, or other contributors to the iteration complexity, to simply the number of function evaluations. Also, the noise in the function values affects the local convergence expectations. Thus, the area is both simpler, in the sense of diminished expectations, and harder, in the sense that one is trying to achieve something with considerably less information. It is definitely fun and challenging and, not incidentally, very useful. Although we do make considerable effort to give a sense of the current state of the art, we do not attempt to present a comprehensive treatise on all the work in the area. This is in part because we think that the subject is not yet mature enough for such a treatise. For similar reasons our emphasis is on the unconstrained problem, although we include a review on the work done so far in constrained derivative-free optimization. The constrained problem is in fact very important for applications, but theoretical treatment of derivative-free methods for constrained problems is very limited in the literature published to date, and thus, for the present volume at least, we are content to concentrate on the unconstrained case.},
  isbn = {978-0-89871-668-9},
  keywords = {algorithms,derivative-free optimization,global convergence,sampling modeling}
}

@article{deisenroth_gaussian_2015,
  title = {Gaussian {{Processes}} for {{Data-Efficient Learning}} in {{Robotics}} and {{Control}}},
  author = {Deisenroth, Marc Peter and Fox, Dieter and Rasmussen, Carl Edward},
  year = {2015},
  month = feb,
  journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  volume = {37},
  number = {2},
  eprint = {1502.02860},
  eprinttype = {arxiv},
  pages = {408--423},
  issn = {0162-8828, 2160-9292},
  doi = {10.1109/TPAMI.2013.218},
  abstract = {Autonomous learning has been a promising direction in control and robotics for more than a decade since data-driven learning allows to reduce the amount of engineering knowledge, which is otherwise required. However, autonomous reinforcement learning (RL) approaches typically require many interactions with the system to learn controllers, which is a practical limitation in real systems, such as robots, where many interactions can be impractical and time consuming. To address this problem, current learning approaches typically require task-specific knowledge in form of expert demonstrations, realistic simulators, pre-shaped policies, or specific knowledge about the underlying dynamics. In this article, we follow a different approach and speed up learning by extracting more information from data. In particular, we learn a probabilistic, non-parametric Gaussian process transition model of the system. By explicitly incorporating model uncertainty into long-term planning and controller learning our approach reduces the effects of model errors, a key problem in model-based learning. Compared to state-of-the art RL our model-based policy search method achieves an unprecedented speed of learning. We demonstrate its applicability to autonomous learning in real robot and control tasks.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning,Computer Science - Robotics,Electrical Engineering and Systems Science - Systems and Control,Statistics - Machine Learning},
  file = {C\:\\Users\\Haggi\\Zotero\\storage\\FPHGT5XD\\Deisenroth et al. - 2015 - Gaussian Processes for Data-Efficient Learning in .pdf;C\:\\Users\\Haggi\\Zotero\\storage\\EH9WSNTB\\1502.html}
}

@inproceedings{fazel_global_2018,
  title = {Global {{Convergence}} of {{Policy Gradient Methods}} for the {{Linear Quadratic Regulator}}},
  booktitle = {Proceedings of the 35th {{International Conference}} on {{Machine Learning}}},
  author = {Fazel, Maryam and Ge, Rong and Kakade, Sham and Mesbahi, Mehran},
  year = {2018},
  month = jul,
  pages = {1467--1476},
  publisher = {{PMLR}},
  issn = {2640-3498},
  abstract = {Direct policy gradient methods for reinforcement learning and continuous control problems are a popular approach for a variety of reasons: 1) they are easy to implement without explicit knowledge of the underlying model, 2) they are an ``end-to-end'' approach, directly optimizing the performance metric of interest, 3) they inherently allow for richly parameterized policies. A notable drawback is that even in the most basic continuous control problem (that of linear quadratic regulators), these methods must solve a non-convex optimization problem, where little is understood about their efficiency from both computational and statistical perspectives. In contrast, system identification and model based planning in optimal control theory have a much more solid theoretical footing, where much is known with regards to their computational and statistical properties. This work bridges this gap showing that (model free) policy gradient methods globally converge to the optimal solution and are efficient (polynomially so in relevant problem dependent quantities) with regards to their sample and computational complexities.},
  langid = {english},
  file = {C\:\\Users\\Haggi\\Zotero\\storage\\DXK256I9\\Fazel et al. - 2018 - Global Convergence of Policy Gradient Methods for .pdf;C\:\\Users\\Haggi\\Zotero\\storage\\DYDKQVZ2\\Fazel et al. - 2018 - Global Convergence of Policy Gradient Methods for .pdf}
}

@incollection{frasconi_linear_2016,
  title = {Linear {{Convergence}} of {{Gradient}} and {{Proximal-Gradient Methods Under}} the {{Polyak-\L ojasiewicz Condition}}},
  booktitle = {Machine {{Learning}} and {{Knowledge Discovery}} in {{Databases}}},
  author = {Karimi, Hamed and Nutini, Julie and Schmidt, Mark},
  editor = {Frasconi, Paolo and Landwehr, Niels and Manco, Giuseppe and Vreeken, Jilles},
  year = {2016},
  volume = {9851},
  pages = {795--811},
  publisher = {{Springer International Publishing}},
  address = {{Cham}},
  doi = {10.1007/978-3-319-46128-1_50},
  isbn = {978-3-319-46127-4 978-3-319-46128-1},
  langid = {english},
  file = {C\:\\Users\\Haggi\\Zotero\\storage\\YNHFU4X3\\Karimi et al. - 2016 - Linear Convergence of Gradient and Proximal-Gradie.pdf}
}

@inproceedings{ipek_self-optimizing_2008,
  title = {Self-{{Optimizing Memory Controllers}}: {{A Reinforcement Learning Approach}}},
  shorttitle = {Self-{{Optimizing Memory Controllers}}},
  booktitle = {2008 {{International Symposium}} on {{Computer Architecture}}},
  author = {Ipek, Engin and Mutlu, Onur and Mart{\'i}nez, Jos{\'e} F. and Caruana, Rich},
  year = {2008},
  month = jun,
  pages = {39--50},
  issn = {1063-6897},
  doi = {10.1109/ISCA.2008.21},
  abstract = {Efficiently utilizing off-chip DRAM bandwidth is a critical issue in designing cost-effective, high-performance chip multiprocessors (CMPs). Conventional memory controllers deliver relatively low performance in part because they often employ fixed, rigid access scheduling policies designed for average-case application behavior. As a result, they cannot learn and optimize the long-term performance impact of their scheduling decisions,and cannot adapt their scheduling policies to dynamic workload behavior.We propose a new, self-optimizing memory controller design that operates using the principles of reinforcement learning (RL)to overcome these limitations. Our RL-based memory controller observes the system state and estimates the long-term performance impact of each action it can take. In this way, the controller learns to optimize its scheduling policy on the fly to maximize long-term performance. Our results show that an RL-based memory controller improves the performance of a set of parallel applications run on a 4-core CMP by 19\% on average (upto 33\%), and it improves DRAM bandwidth utilization by 22\%compared to a state-of-the-art controller.},
  keywords = {Automatic control,Bandwidth,Chip Multiprocessors,Control systems,Delay,Dynamic scheduling,Job shop scheduling,Machine learning,Machine Learning,Memory Controller,Memory Systems,Moore's Law,Random access memory,Reinforcement Learning,State estimation},
  file = {C\:\\Users\\Haggi\\Zotero\\storage\\QBCMXB2G\\Ipek et al. - 2008 - Self-Optimizing Memory Controllers A Reinforcemen.pdf;C\:\\Users\\Haggi\\Zotero\\storage\\4IYJIID4\\4556714.html}
}

@article{janner_when_2019,
  title = {When to {{Trust Your Model}}: {{Model-Based Policy Optimization}}},
  shorttitle = {When to {{Trust Your Model}}},
  author = {Janner, Michael and Fu, Justin and Zhang, Marvin and Levine, Sergey},
  year = {2019},
  month = nov,
  journal = {arXiv:1906.08253 [cs, stat]},
  eprint = {1906.08253},
  eprinttype = {arxiv},
  primaryclass = {cs, stat},
  abstract = {Designing effective model-based reinforcement learning algorithms is difficult because the ease of data generation must be weighed against the bias of model-generated data. In this paper, we study the role of model usage in policy optimization both theoretically and empirically. We first formulate and analyze a model-based reinforcement learning algorithm with a guarantee of monotonic improvement at each step. In practice, this analysis is overly pessimistic and suggests that real off-policy data is always preferable to model-generated on-policy data, but we show that an empirical estimate of model generalization can be incorporated into such analysis to justify model usage. Motivated by this analysis, we then demonstrate that a simple procedure of using short model-generated rollouts branched from real data has the benefits of more complicated model-based algorithms without the usual pitfalls. In particular, this approach surpasses the sample efficiency of prior model-based methods, matches the asymptotic performance of the best model-free algorithms, and scales to horizons that cause other model-based methods to fail entirely.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {C\:\\Users\\Haggi\\Zotero\\storage\\DYBBE9YU\\Janner et al. - 2019 - When to Trust Your Model Model-Based Policy Optim.pdf}
}

@inproceedings{johannink_residual_2019,
  title = {Residual {{Reinforcement Learning}} for {{Robot Control}}},
  booktitle = {2019 {{International Conference}} on {{Robotics}} and {{Automation}} ({{ICRA}})},
  author = {Johannink, Tobias and Bahl, Shikhar and Nair, Ashvin and Luo, Jianlan and Kumar, Avinash and Loskyll, Matthias and Ojea, Juan Aparicio and Solowjow, Eugen and Levine, Sergey},
  year = {2019},
  month = may,
  pages = {6023--6029},
  issn = {2577-087X},
  doi = {10.1109/ICRA.2019.8794127},
  abstract = {Conventional feedback control methods can solve various types of robot control problems very efficiently by capturing the structure with explicit models, such as rigid body equations of motion. However, many control problems in modern manufacturing deal with contacts and friction, which are difficult to capture with first-order physical modeling. Hence, applying control design methodologies to these kinds of problems often results in brittle and inaccurate controllers, which have to be manually tuned for deployment. Reinforcement learning (RL) methods have been demonstrated to be capable of learning continuous robot controllers from interactions with the environment, even for problems that include friction and contacts. In this paper, we study how we can solve difficult control problems in the real world by decomposing them into a part that is solved efficiently by conventional feedback control methods, and the residual which is solved with RL. The final control policy is a superposition of both control signals. We demonstrate our approach by training an agent to successfully perform a real-world block assembly task involving contacts and unstable objects.},
  keywords = {Adaptive control,Feedback control,Manufacturing,Mathematical model,Reinforcement learning,Robots,Task analysis},
  file = {C\:\\Users\\Haggi\\Zotero\\storage\\5ESXWHNW\\8794127.html}
}

@inproceedings{kohl_policy_2004,
  title = {Policy Gradient Reinforcement Learning for Fast Quadrupedal Locomotion},
  booktitle = {{{IEEE International Conference}} on {{Robotics}} and {{Automation}}, 2004. {{Proceedings}}. {{ICRA}} '04. 2004},
  author = {Kohl, N. and Stone, P.},
  year = {2004},
  month = apr,
  volume = {3},
  pages = {2619-2624 Vol.3},
  issn = {1050-4729},
  doi = {10.1109/ROBOT.2004.1307456},
  abstract = {This paper presents a machine learning approach to optimizing a quadrupedal trot gait for forward speed. Given a parameterized walk designed for a specific robot, we propose using a form of policy gradient reinforcement learning to automatically search the set of possible parameters with the goal of finding the fastest possible walk. We implement and test our approach on a commercially available quadrupedal robot platform, namely the Sony Aibo robot. After about three hours of learning, all on the physical robots and with no human intervention other than to change the batteries, the robots achieved a gait faster than any previously known gait known for the Aibo, significantly outperforming a variety of existing hand-coded and learned solutions.},
  keywords = {Friction,Hardware,Humans,Leg,Legged locomotion,Machine learning,Robot control,Robotics and automation,Stability,Testing},
  file = {C\:\\Users\\Haggi\\Zotero\\storage\\69V2EPBZ\\Kohl and Stone - 2004 - Policy gradient reinforcement learning for fast qu.pdf;C\:\\Users\\Haggi\\Zotero\\storage\\APSD6C2P\\1307456.html}
}

@article{lecun_gradient-based_1998,
  title = {Gradient-Based Learning Applied to Document Recognition},
  author = {Lecun, Y. and Bottou, L. and Bengio, Y. and Haffner, P.},
  year = {1998},
  month = nov,
  journal = {Proceedings of the IEEE},
  volume = {86},
  number = {11},
  pages = {2278--2324},
  issn = {1558-2256},
  doi = {10.1109/5.726791},
  abstract = {Multilayer neural networks trained with the back-propagation algorithm constitute the best example of a successful gradient based learning technique. Given an appropriate network architecture, gradient-based learning algorithms can be used to synthesize a complex decision surface that can classify high-dimensional patterns, such as handwritten characters, with minimal preprocessing. This paper reviews various methods applied to handwritten character recognition and compares them on a standard handwritten digit recognition task. Convolutional neural networks, which are specifically designed to deal with the variability of 2D shapes, are shown to outperform all other techniques. Real-life document recognition systems are composed of multiple modules including field extraction, segmentation recognition, and language modeling. A new learning paradigm, called graph transformer networks (GTN), allows such multimodule systems to be trained globally using gradient-based methods so as to minimize an overall performance measure. Two systems for online handwriting recognition are described. Experiments demonstrate the advantage of global training, and the flexibility of graph transformer networks. A graph transformer network for reading a bank cheque is also described. It uses convolutional neural network character recognizers combined with global training techniques to provide record accuracy on business and personal cheques. It is deployed commercially and reads several million cheques per day.},
  keywords = {Character recognition,Feature extraction,Hidden Markov models,Machine learning,Multi-layer neural network,Neural networks,Optical character recognition software,Optical computing,Pattern recognition,Principal component analysis},
  file = {C\:\\Users\\Haggi\\Zotero\\storage\\N8I8HVM7\\Lecun et al. - 1998 - Gradient-based learning applied to document recogn.pdf;C\:\\Users\\Haggi\\Zotero\\storage\\EQ264MPF\\726791.html}
}

@inproceedings{li_contextual-bandit_2010,
  title = {A Contextual-Bandit Approach to Personalized News Article Recommendation},
  booktitle = {Proceedings of the 19th International Conference on {{World}} Wide Web - {{WWW}} '10},
  author = {Li, Lihong and Chu, Wei and Langford, John and Schapire, Robert E.},
  year = {2010},
  pages = {661},
  publisher = {{ACM Press}},
  address = {{Raleigh, North Carolina, USA}},
  doi = {10.1145/1772690.1772758},
  isbn = {978-1-60558-799-8},
  langid = {english}
}

@article{mnih_human-level_2015,
  title = {Human-Level Control through Deep Reinforcement Learning},
  author = {Mnih, Volodymyr and Kavukcuoglu, Koray and Silver, David and Rusu, Andrei A. and Veness, Joel and Bellemare, Marc G. and Graves, Alex and Riedmiller, Martin and Fidjeland, Andreas K. and Ostrovski, Georg and Petersen, Stig and Beattie, Charles and Sadik, Amir and Antonoglou, Ioannis and King, Helen and Kumaran, Dharshan and Wierstra, Daan and Legg, Shane and Hassabis, Demis},
  year = {2015},
  month = feb,
  journal = {Nature},
  volume = {518},
  number = {7540},
  pages = {529--533},
  issn = {0028-0836, 1476-4687},
  doi = {10.1038/nature14236},
  langid = {english}
}

@book{nocedal_numerical_2006,
  title = {Numerical Optimization},
  author = {Nocedal, Jorge and Wright, Stephen J.},
  year = {2006},
  series = {Springer Series in Operations Research and Financial Engineering},
  edition = {2nd ed},
  publisher = {{Springer}},
  address = {{New York}},
  abstract = {'Numerical Optimization' presents a comprehensive description of the effective methods in continuous optimization. The book includes chapters on nonlinear interior methods \& derivative-free methods for optimization. It is useful for graduate students, researchers and practitioners},
  isbn = {978-0-387-40065-5},
  langid = {english},
  lccn = {519.6}
}

@article{parikh_proximal_2014,
  title = {Proximal {{Algorithms}}},
  author = {Parikh, Neal and Boyd, Stephen},
  year = {2014},
  month = jan,
  journal = {Foundations and Trends\textregistered{} in Optimization},
  volume = {1},
  number = {3},
  pages = {127--239},
  publisher = {{Now Publishers, Inc.}},
  issn = {2167-3888, 2167-3918},
  doi = {10.1561/2400000003},
  abstract = {Proximal Algorithms},
  langid = {english},
  file = {C\:\\Users\\Haggi\\Zotero\\storage\\XND4PALJ\\Parikh and Boyd - 2014 - Proximal Algorithms.pdf;C\:\\Users\\Haggi\\Zotero\\storage\\Q75NQZ6S\\OPT-003.html}
}

@article{qu_combining_2020,
  title = {Combining {{Model-Based}} and {{Model-Free Methods}} for {{Nonlinear Control}}: {{A Provably Convergent Policy Gradient Approach}}},
  shorttitle = {Combining {{Model-Based}} and {{Model-Free Methods}} for {{Nonlinear Control}}},
  author = {Qu, Guannan and Yu, Chenkai and Low, Steven and Wierman, Adam},
  year = {2020},
  month = jun,
  journal = {arXiv:2006.07476 [cs, eess, math]},
  eprint = {2006.07476},
  eprinttype = {arxiv},
  primaryclass = {cs, eess, math},
  abstract = {Model-free learning-based control methods have seen great success recently. However, such methods typically suffer from poor sample complexity and limited convergence guarantees. This is in sharp contrast to classical model-based control, which has a rich theory but typically requires strong modeling assumptions. In this paper, we combine the two approaches to achieve the best of both worlds. We consider a dynamical system with both linear and non-linear components and develop a novel approach to use the linear model to define a warm start for a model-free, policy gradient method. We show this hybrid approach outperforms the model-based controller while avoiding the convergence issues associated with model-free approaches via both numerical experiments and theoretical analyses, in which we derive sufficient conditions on the non-linear component such that our approach is guaranteed to converge to the (nearly) global optimal controller.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning,Electrical Engineering and Systems Science - Systems and Control,Mathematics - Optimization and Control},
  file = {C\:\\Users\\Haggi\\Zotero\\storage\\D26DQFQB\\Qu et al. - 2020 - Combining Model-Based and Model-Free Methods for N.pdf}
}

@article{rockafellar_monotone_1976,
  title = {Monotone {{Operators}} and the {{Proximal Point Algorithm}}},
  author = {Rockafellar, R. Tyrrell},
  year = {1976},
  month = aug,
  journal = {SIAM Journal on Control and Optimization},
  volume = {14},
  number = {5},
  pages = {877--898},
  publisher = {{Society for Industrial and Applied Mathematics}},
  issn = {0363-0129},
  doi = {10.1137/0314056},
  abstract = {For the problem of minimizing a lower semicontinuous proper convex function f on a Hilbert space, the proximal point algorithm in exact form generates a sequence \$\textbackslash\{ z\^k \textbackslash\} \$ by taking \$z\^\{k + 1\} \$ to be the minimizes of \$f(z) + (\{1 / \{2c\_k \}\})\textbackslash | \{z - z\^k \} \textbackslash |\^2 \$, where \$c\_k {$>$} 0\$. This algorithm is of interest for several reasons, but especially because of its role in certain computational methods based on duality, such as the Hestenes-Powell method of multipliers in nonlinear programming. It is investigated here in a more general form where the requirement for exact minimization at each iteration is weakened, and the subdifferential \$\textbackslash partial f\$ is replaced by an arbitrary maximal monotone operator T. Convergence is established under several criteria amenable to implementation. The rate of convergence is shown to be ``typically'' linear with an arbitrarily good modulus if \$c\_k \$ stays large enough, in fact superlinear if \$c\_k \textbackslash to \textbackslash infty \$. The case of \$T = \textbackslash partial f\$ is treated in extra detail. Application is also made to a related case corresponding to minimax problems.}
}

@inproceedings{sedighizadeh_self-tuning_2008-1,
  title = {A Self-Tuning {{PID}} Control for a Wind Energy Conversion System Based on the {{Lyapunov}} Approach},
  booktitle = {2008 43rd {{International Universities Power Engineering Conference}}},
  author = {Sedighizadeh, M. and Rezazadeh, A. and Khatibi, M.},
  year = {2008},
  month = sep,
  pages = {1--4},
  publisher = {{IEEE}},
  address = {{Padova}},
  doi = {10.1109/UPEC.2008.4651560},
  isbn = {978-1-4244-3294-3 978-88-89884-09-6}
}

@article{shah_airsim_2017,
  title = {{{AirSim}}: {{High-Fidelity Visual}} and {{Physical Simulation}} for {{Autonomous Vehicles}}},
  shorttitle = {{{AirSim}}},
  author = {Shah, Shital and Dey, Debadeepta and Lovett, Chris and Kapoor, Ashish},
  year = {2017},
  month = jul,
  journal = {arXiv:1705.05065 [cs]},
  eprint = {1705.05065},
  eprinttype = {arxiv},
  primaryclass = {cs},
  abstract = {Developing and testing algorithms for autonomous vehicles in real world is an expensive and time consuming process. Also, in order to utilize recent advances in machine intelligence and deep learning we need to collect a large amount of annotated training data in a variety of conditions and environments. We present a new simulator built on Unreal Engine that offers physically and visually realistic simulations for both of these goals. Our simulator includes a physics engine that can operate at a high frequency for real-time hardware-in-the-loop (HITL) simulations with support for popular protocols (e.g. MavLink). The simulator is designed from the ground up to be extensible to accommodate new types of vehicles, hardware platforms and software protocols. In addition, the modular design enables various components to be easily usable independently in other projects. We demonstrate the simulator by first implementing a quadrotor as an autonomous vehicle and then experimentally comparing the software components with real-world flights.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computer Vision and Pattern Recognition,Computer Science - Robotics,Electrical Engineering and Systems Science - Systems and Control},
  file = {C\:\\Users\\Haggi\\Zotero\\storage\\JD9KM4DF\\Shah et al. - 2017 - AirSim High-Fidelity Visual and Physical Simulati.pdf;C\:\\Users\\Haggi\\Zotero\\storage\\S6CJN68Y\\1705.html}
}

@inproceedings{shashua_sim_2021-1,
  title = {Sim and {{Real}}: {{Better Together}}},
  shorttitle = {Sim and {{Real}}},
  booktitle = {Thirty-{{Fifth Conference}} on {{Neural Information Processing Systems}}},
  author = {Shashua, Shirli Di-Castro and Castro, Dotan Di and Mannor, Shie},
  year = {2021},
  month = may,
  abstract = {We provide an algorithm and a novel replay buffer analysis for sim to real in robotics application},
  langid = {english},
  file = {C\:\\Users\\Haggi\\Zotero\\storage\\3SB63VGB\\Shashua et al. - 2021 - Sim and Real Better Together.pdf;C\:\\Users\\Haggi\\Zotero\\storage\\YA2XHFPC\\forum.html}
}

@article{silver_mastering_2016,
  title = {Mastering the Game of {{Go}} with Deep Neural Networks and Tree Search},
  author = {Silver, David and Huang, Aja and Maddison, Chris J. and Guez, Arthur and Sifre, Laurent and {van den Driessche}, George and Schrittwieser, Julian and Antonoglou, Ioannis and Panneershelvam, Veda and Lanctot, Marc and Dieleman, Sander and Grewe, Dominik and Nham, John and Kalchbrenner, Nal and Sutskever, Ilya and Lillicrap, Timothy and Leach, Madeleine and Kavukcuoglu, Koray and Graepel, Thore and Hassabis, Demis},
  year = {2016},
  month = jan,
  journal = {Nature},
  volume = {529},
  number = {7587},
  pages = {484--489},
  publisher = {{Nature Publishing Group}},
  issn = {1476-4687},
  doi = {10.1038/nature16961},
  abstract = {The game of Go has long been viewed as the most challenging of classic games for artificial intelligence owing to its enormous search space and the difficulty of evaluating board positions and moves. Here we introduce a new approach to computer Go that uses `value networks' to evaluate board positions and `policy networks' to select moves. These deep neural networks are trained by a novel combination of supervised learning from human expert games, and reinforcement learning from games of self-play. Without any lookahead search, the neural networks play Go at the level of state-of-the-art Monte Carlo tree search programs that simulate thousands of random games of self-play. We also introduce a new search algorithm that combines Monte Carlo simulation with value and policy networks. Using this search algorithm, our program AlphaGo achieved a 99.8\% winning rate against other Go programs, and defeated the human European Go champion by 5 games to 0. This is the first time that a computer program has defeated a human professional player in the full-sized game of Go, a feat previously thought to be at least a decade away.},
  copyright = {2016 Nature Publishing Group, a division of Macmillan Publishers Limited. All Rights Reserved.},
  langid = {english},
  keywords = {Computational science,Computer science,Reward},
  annotation = {Bandiera\_abtest: a Cg\_type: Nature Research Journals Primary\_atype: Research Subject\_term: Computational science;Computer science;Reward Subject\_term\_id: computational-science;computer-science;reward},
  file = {C\:\\Users\\Haggi\\Zotero\\storage\\TG4V9DHF\\nature16961.html}
}

@article{silver_mastering_2017,
  title = {Mastering the Game of {{Go}} without Human Knowledge},
  author = {Silver, David and Schrittwieser, Julian and Simonyan, Karen and Antonoglou, Ioannis and Huang, Aja and Guez, Arthur and Hubert, Thomas and Baker, Lucas and Lai, Matthew and Bolton, Adrian and Chen, Yutian and Lillicrap, Timothy and Hui, Fan and Sifre, Laurent and {van den Driessche}, George and Graepel, Thore and Hassabis, Demis},
  year = {2017},
  month = oct,
  journal = {Nature},
  volume = {550},
  number = {7676},
  pages = {354--359},
  publisher = {{Nature Publishing Group}},
  issn = {1476-4687},
  doi = {10.1038/nature24270},
  abstract = {A long-standing goal of artificial intelligence is an algorithm that learns, tabula rasa, superhuman proficiency in challenging domains. Recently, AlphaGo became the first program to defeat a world champion in the game of Go. The tree search in AlphaGo evaluated positions and selected moves using deep neural networks. These neural networks were trained by supervised learning from human expert moves, and by reinforcement learning from self-play. Here we introduce an algorithm based solely on reinforcement learning, without human data, guidance or domain knowledge beyond game rules. AlphaGo becomes its own teacher: a neural network is trained to predict AlphaGo's own move selections and also the winner of AlphaGo's games. This neural network improves the strength of the tree search, resulting in higher quality move selection and stronger self-play in the next iteration. Starting tabula rasa, our new program AlphaGo Zero achieved superhuman performance, winning 100\textendash 0 against the previously published, champion-defeating AlphaGo.},
  copyright = {2017 Macmillan Publishers Limited, part of Springer Nature. All rights reserved.},
  langid = {english},
  keywords = {Computational science,Computer science,Reward},
  annotation = {Bandiera\_abtest: a Cg\_type: Nature Research Journals Primary\_atype: Research Subject\_term: Computational science;Computer science;Reward Subject\_term\_id: computational-science;computer-science;reward},
  file = {C\:\\Users\\Haggi\\Zotero\\storage\\VWJDCMHX\\Silver et al. - 2017 - Mastering the game of Go without human knowledge.pdf;C\:\\Users\\Haggi\\Zotero\\storage\\XWUCL9IT\\nature24270.html}
}

@inproceedings{tu_gap_2019-1,
  title = {The {{Gap Between Model-Based}} and {{Model-Free Methods}} on the {{Linear Quadratic Regulator}}: {{An Asymptotic Viewpoint}}},
  shorttitle = {The {{Gap Between Model-Based}} and {{Model-Free Methods}} on the {{Linear Quadratic Regulator}}},
  booktitle = {Proceedings of the {{Thirty-Second Conference}} on {{Learning Theory}}},
  author = {Tu, Stephen and Recht, Benjamin},
  year = {2019},
  month = jun,
  pages = {3036--3083},
  publisher = {{PMLR}},
  issn = {2640-3498},
  abstract = {The effectiveness of model-based versus model-free methods is a long-standing question in reinforcement learning (RL).  Motivated by recent empirical success of RL on continuous control tasks, we study the sample complexity of popular model-based and model-free algorithms on the Linear Quadratic Regulator (LQR).  We show that for policy evaluation, a simple model-based plugin method requires asymptotically less samples than the classical least-squares temporal difference (LSTD) estimator to reach the same quality of solution; the sample complexity gap between the two methods can be at least a factor of state dimension.  For policy optimization, we study a simple family of problem instances and show that nominal (certainty equivalence principle) control also requires several factors of state and input dimension fewer samples than the policy gradient method to reach the same level of control performance on these instances.  Furthermore, the gap persists even when employing baselines commonly used in practice.  To the best of our knowledge, this is the first theoretical result which demonstrates a separation in the sample complexity between model-based and model-free methods on a continuous control task.},
  langid = {english},
  file = {C\:\\Users\\Haggi\\Zotero\\storage\\5JQ68HKN\\Tu and Recht - 2019 - The Gap Between Model-Based and Model-Free Methods.pdf}
}



@article{chen_reinforcement_2021,
  title = {Reinforcement {{Learning}} for {{Decision-Making}} and {{Control}} in {{Power Systems}}: {{Tutorial}}, {{Review}}, and {{Vision}}},
  shorttitle = {Reinforcement {{Learning}} for {{Decision-Making}} and {{Control}} in {{Power Systems}}},
  author = {Chen, Xin and Qu, Guannan and Tang, Yujie and Low, Steven and Li, Na},
  year = {2021},
  journal = {arXiv:2102.01168 [cs, eess]},
  eprint = {2102.01168},
  eprinttype = {arxiv},
  primaryclass = {cs, eess},
  archiveprefix = {arXiv}
}

@article{kober_reinforcement_2013,
  title = {Reinforcement Learning in Robotics: {{A}} Survey},
  shorttitle = {Reinforcement Learning in Robotics},
  author = {Kober, Jens and Bagnell, J. Andrew and Peters, Jan},
  year = {2013},
  journal = {The International Journal of Robotics Research},
  volume = {32},
  number = {11},
  pages = {1238--1274},
  publisher = {{SAGE Publications Ltd STM}},
  issn = {0278-3649},
  doi = {10.1177/0278364913495721}
}

@article{levine_end--end_2016,
  title = {End-to-End Training of Deep Visuomotor Policies},
  author = {Levine, Sergey and Finn, Chelsea and Darrell, Trevor and Abbeel, Pieter},
  year = {2016},
  journal = {Journal of Machine Learning Research},
  volume = {17},
  number = {39},
  pages = {1--40}
}

@article{lewis_proximal_2016,
  title = {A Proximal Method for Composite Minimization},
  author = {Lewis, A. S. and Wright, S. J.},
  year = {2016},
  journal = {Mathematical Programming},
  volume = {158},
  number = {1-2},
  pages = {501--546},
  issn = {0025-5610, 1436-4646},
  doi = {10.1007/s10107-015-0943-9},
  langid = {english}
}

@article{nesterov_gradient_2013,
  title = {Gradient Methods for Minimizing Composite Functions},
  author = {Nesterov, Yu.},
  year = {2013},
  journal = {Mathematical Programming},
  volume = {140},
  number = {1},
  pages = {125--161},
  issn = {0025-5610, 1436-4646},
  doi = {10.1007/s10107-012-0629-5},
  langid = {english}
}

@article{recht_tour_2019,
  title = {A {{Tour}} of {{Reinforcement Learning}}: {{The View}} from {{Continuous Control}}},
  shorttitle = {A {{Tour}} of {{Reinforcement Learning}}},
  author = {Recht, Benjamin},
  year = {2019},
  journal = {Annual Review of Control, Robotics, and Autonomous Systems},
  volume = {2},
  number = {1},
  pages = {253--279},
  doi = {10.1146/annurev-control-053018-023825},
  annotation = {\_eprint: https://doi.org/10.1146/annurev-control-053018-023825}
}

@article{wu_flow_2017,
  title = {Flow: {{Architecture}} and {{Benchmarking}} for {{Reinforcement Learning}} in {{Traffic Control}}},
  shorttitle = {Flow},
  author = {Wu, Cathy and Kreidieh, Aboudy and Parvate, Kanaad and Vinitsky, Eugene and Bayen, Alexandre M.},
  year = {2017},
  journal = {arXiv:1710.05465 [cs]},
  eprint = {1710.05465},
  eprinttype = {arxiv},
  primaryclass = {cs},
  archiveprefix = {arXiv}
}


