@misc{liu_one_2023,
  title = {One {{Objective}} to {{Rule Them All}}: {{A Maximization Objective Fusing Estimation}} and {{Planning}} for {{Exploration}}},
  shorttitle = {One {{Objective}} to {{Rule Them All}}},
  author = {Liu, Zhihan and Lu, Miao and Xiong, Wei and Zhong, Han and Hu, Hao and Zhang, Shenao and Zheng, Sirui and Yang, Zhuoran and Wang, Zhaoran},
  year = {2023},
  month = may,
  number = {arXiv:2305.18258},
  eprint = {2305.18258},
  primaryclass = {cs, math, stat},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.2305.18258},
  urldate = {2023-06-03},
  abstract = {In online reinforcement learning (online RL), balancing exploration and exploitation is crucial for finding an optimal policy in a sample-efficient way. To achieve this, existing sample-efficient online RL algorithms typically consist of three components: estimation, planning, and exploration. However, in order to cope with general function approximators, most of them involve impractical algorithmic components to incentivize exploration, such as optimization within data-dependent level-sets or complicated sampling procedures. To address this challenge, we propose an easy-to-implement RL framework called \textbackslash textit\{Maximize to Explore\} (\textbackslash texttt\{MEX\}), which only needs to optimize \textbackslash emph\{unconstrainedly\} a single objective that integrates the estimation and planning components while balancing exploration and exploitation automatically. Theoretically, we prove that \textbackslash texttt\{MEX\} achieves a sublinear regret with general function approximations for Markov decision processes (MDP) and is further extendable to two-player zero-sum Markov games (MG). Meanwhile, we adapt deep RL baselines to design practical versions of \textbackslash texttt\{MEX\}, in both model-free and model-based manners, which can outperform baselines by a stable margin in various MuJoCo environments with sparse rewards. Compared with existing sample-efficient online RL algorithms with general function approximations, \textbackslash texttt\{MEX\} achieves similar sample efficiency while enjoying a lower computational cost and is more compatible with modern deep RL methods.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computer Science and Game Theory,Computer Science - Machine Learning,Mathematics - Optimization and Control,Statistics - Machine Learning},
  file = {C\:\\Users\\Haggi\\Zotero\\storage\\DPZN5XT7\\Liu et al. - 2023 - One Objective to Rule Them All A Maximization Obj.pdf;C\:\\Users\\Haggi\\Zotero\\storage\\U3IZXPTD\\2305.html}
}
