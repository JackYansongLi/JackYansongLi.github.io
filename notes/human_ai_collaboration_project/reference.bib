@misc{agarwal_model-based_2022,
  title = {Model-Based {{RL}} with {{Optimistic Posterior Sampling}}: {{Structural Conditions}} and {{Sample Complexity}}},
  shorttitle = {Model-Based {{RL}} with {{Optimistic Posterior Sampling}}},
  author = {Agarwal, Alekh and Zhang, Tong},
  year = {2022},
  month = oct,
  number = {arXiv:2206.07659},
  eprint = {2206.07659},
  primaryclass = {cs, stat},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.2206.07659},
  urldate = {2023-06-27},
  abstract = {We propose a general framework to design posterior sampling methods for model-based RL. We show that the proposed algorithms can be analyzed by reducing regret to Hellinger distance in conditional probability estimation. We further show that optimistic posterior sampling can control this Hellinger distance, when we measure model error via data likelihood. This technique allows us to design and analyze unified posterior sampling algorithms with state-of-the-art sample complexity guarantees for many model-based RL settings. We illustrate our general result in many special cases, demonstrating the versatility of our framework.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/Users/yansongli/zotero/storage/D3BV7DAZ/Agarwal and Zhang - 2022 - Model-based RL with Optimistic Posterior Sampling.pdf;/Users/yansongli/zotero/storage/IHI3R9X2/2206.html}
}

@article{albrecht_belief_2016,
  title = {Belief and {{Truth}} in {{Hypothesised Behaviours}}},
  author = {Albrecht, Stefano V. and Crandall, Jacob W. and Ramamoorthy, Subramanian},
  year = {2016},
  month = jun,
  journal = {Artificial Intelligence},
  volume = {235},
  eprint = {1507.07688},
  primaryclass = {cs},
  pages = {63--94},
  issn = {00043702},
  doi = {10.1016/j.artint.2016.02.004},
  urldate = {2023-04-30},
  abstract = {There is a long history in game theory on the topic of Bayesian or "rational" learning, in which each player maintains beliefs over a set of alternative behaviours, or types, for the other players. This idea has gained increasing interest in the artificial intelligence (AI) community, where it is used as a method to control a single agent in a system composed of multiple agents with unknown behaviours. The idea is to hypothesise a set of types, each specifying a possible behaviour for the other agents, and to plan our own actions with respect to those types which we believe are most likely, given the observed actions of the agents. The game theory literature studies this idea primarily in the context of equilibrium attainment. In contrast, many AI applications have a focus on task completion and payoff maximisation. With this perspective in mind, we identify and address a spectrum of questions pertaining to belief and truth in hypothesised types. We formulate three basic ways to incorporate evidence into posterior beliefs and show when the resulting beliefs are correct, and when they may fail to be correct. Moreover, we demonstrate that prior beliefs can have a significant impact on our ability to maximise payoffs in the long-term, and that they can be computed automatically with consistent performance effects. Furthermore, we analyse the conditions under which we are able complete our task optimally, despite inaccuracies in the hypothesised types. Finally, we show how the correctness of hypothesised types can be ascertained during the interaction via an automated statistical analysis.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computer Science and Game Theory,I.2.11},
  file = {/Users/yansongli/zotero/storage/Y7VXWSQA/Albrecht et al. - 2016 - Belief and Truth in Hypothesised Behaviours.pdf;/Users/yansongli/zotero/storage/KEN4KWIM/1507.html}
}

@misc{albrecht_game-theoretic_2015,
  title = {A {{Game-Theoretic Model}} and {{Best-Response Learning Method}} for {{Ad Hoc Coordination}} in {{Multiagent Systems}}},
  author = {Albrecht, Stefano V. and Ramamoorthy, Subramanian},
  year = {2015},
  month = jun,
  number = {arXiv:1506.01170},
  eprint = {1506.01170},
  primaryclass = {cs},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.1506.01170},
  urldate = {2023-04-30},
  abstract = {The ad hoc coordination problem is to design an autonomous agent which is able to achieve optimal flexibility and efficiency in a multiagent system with no mechanisms for prior coordination. We conceptualise this problem formally using a game-theoretic model, called the stochastic Bayesian game, in which the behaviour of a player is determined by its private information, or type. Based on this model, we derive a solution, called Harsanyi-Bellman Ad Hoc Coordination (HBA), which utilises the concept of Bayesian Nash equilibrium in a planning procedure to find optimal actions in the sense of Bellman optimal control. We evaluate HBA in a multiagent logistics domain called level-based foraging, showing that it achieves higher flexibility and efficiency than several alternative algorithms. We also report on a human-machine experiment at a public science exhibition in which the human participants played repeated Prisoner's Dilemma and Rock-Paper-Scissors against HBA and alternative algorithms, showing that HBA achieves equal efficiency and a significantly higher welfare and winning rate.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computer Science and Game Theory,Computer Science - Multiagent Systems},
  file = {/Users/yansongli/zotero/storage/BKEGCN6F/Albrecht and Ramamoorthy - 2015 - A Game-Theoretic Model and Best-Response Learning .pdf;/Users/yansongli/zotero/storage/CYAM46GI/1506.html}
}

@article{barrett_making_2017,
  title = {Making Friends on the Fly: {{Cooperating}} with New Teammates},
  shorttitle = {Making Friends on the Fly},
  author = {Barrett, Samuel and Rosenfeld, Avi and Kraus, Sarit and Stone, Peter},
  year = {2017},
  month = jan,
  journal = {Artificial Intelligence},
  volume = {242},
  pages = {132--171},
  issn = {00043702},
  doi = {10.1016/j.artint.2016.10.005},
  urldate = {2023-04-30},
  langid = {english},
  file = {/Users/yansongli/zotero/storage/9R3LU8YF/Barrett et al. - 2017 - Making friends on the fly Cooperating with new te.pdf;/Users/yansongli/zotero/storage/ARWP2YG4/Barrett et al. - 2017 - Making friends on the fly Cooperating with new te.pdf}
}

@incollection{baumeister_survey_2022,
  title = {A {{Survey}} of {{Ad Hoc Teamwork Research}}},
  booktitle = {Multi-{{Agent Systems}}},
  author = {Mirsky, Reuth and Carlucho, Ignacio and Rahman, Arrasy and Fosong, Elliot and Macke, William and Sridharan, Mohan and Stone, Peter and Albrecht, Stefano V.},
  editor = {Baumeister, Dorothea and Rothe, J{\"o}rg},
  year = {2022},
  volume = {13442},
  pages = {275--293},
  publisher = {{Springer International Publishing}},
  address = {{Cham}},
  doi = {10.1007/978-3-031-20614-6_16},
  urldate = {2023-04-27},
  abstract = {Ad hoc teamwork is the research problem of designing agents that can collaborate with new teammates without prior coordination. This survey makes a two-fold contribution: First, it provides a structured description of the different facets of the ad hoc teamwork problem. Second, it discusses the progress that has been made in the field so far, and identifies the immediate and long-term open problems that need to be addressed in ad hoc teamwork.},
  isbn = {978-3-031-20613-9 978-3-031-20614-6},
  langid = {english},
  file = {/Users/yansongli/zotero/storage/9YM8QPDM/Mirsky et al. - 2022 - A Survey of Ad Hoc Teamwork Research.pdf}
}

@misc{brunskill_sample_2013,
  title = {Sample {{Complexity}} of {{Multi-task Reinforcement Learning}}},
  author = {Brunskill, Emma and Li, Lihong},
  year = {2013},
  month = sep,
  number = {arXiv:1309.6821},
  eprint = {1309.6821},
  primaryclass = {cs, stat},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.1309.6821},
  urldate = {2023-05-25},
  abstract = {Transferring knowledge across a sequence of reinforcement-learning tasks is challenging, and has a number of important applications. Though there is encouraging empirical evidence that transfer can improve performance in subsequent reinforcement-learning tasks, there has been very little theoretical analysis. In this paper, we introduce a new multi-task algorithm for a sequence of reinforcement-learning tasks when each task is sampled independently from (an unknown) distribution over a finite set of Markov decision processes whose parameters are initially unknown. For this setting, we prove under certain assumptions that the per-task sample complexity of exploration is reduced significantly due to transfer compared to standard single-task algorithms. Our multi-task algorithm also has the desired characteristic that it is guaranteed not to exhibit negative transfer: in the worst case its per-task sample complexity is comparable to the corresponding single-task algorithm.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/Users/yansongli/zotero/storage/WL2B6WN7/Brunskill and Li - 2013 - Sample Complexity of Multi-task Reinforcement Lear.pdf;/Users/yansongli/zotero/storage/KPQ7D685/1309.html}
}

@article{bubeck_regret_2012,
  title = {Regret {{Analysis}} of {{Stochastic}} and {{Nonstochastic Multi-armed Bandit Problems}}},
  author = {Bubeck, S{\'e}bastien},
  year = {2012},
  journal = {FNT in Machine Learning},
  volume = {5},
  number = {1},
  pages = {1--122},
  issn = {1935-8237, 1935-8245},
  doi = {10.1561/2200000024},
  urldate = {2023-06-28},
  abstract = {Multi-armed bandit problems are the most basic examples of sequential decision problems with an exploration\textendash exploitation trade-off. This is the balance between staying with the option that gave highest payoffs in the past and exploring new options that might give higher payoffs in the future. Although the study of bandit problems dates back to the 1930s, exploration\textendash exploitation trade-offs arise in several modern applications, such as ad placement, website optimization, and packet routing. Mathematically, a multi-armed bandit is defined by the payoff process associated with each option. In this monograph, we focus on two extreme cases in which the analysis of regret is particularly simple and elegant: i.i.d. payoffs and adversarial payoffs. Besides the basic setting of finitely many actions, we also analyze some of the most important variants and extensions, such as the contextual bandit model.},
  langid = {english},
  file = {/Users/yansongli/zotero/storage/3BEHYJ5P/Bubeck - 2012 - Regret Analysis of Stochastic and Nonstochastic Mu.pdf}
}

@article{carroll_utility_2019,
  title = {On the {{Utility}} of {{Learning}} about {{Humans}} for {{Human-AI Coordination}}},
  author = {Carroll, Micah and Shah, Rohin and Ho, Mark K. and Griffiths, Thomas L. and Seshia, Sanjit A. and Abbeel, Pieter and Dragan, Anca},
  year = {2019},
  month = oct,
  doi = {10.48550/arXiv.1910.05789},
  urldate = {2022-10-11},
  abstract = {While we would like agents that can coordinate with humans, current algorithms such as self-play and population-based training create agents that can coordinate with themselves. Agents that assume their partner to be optimal or similar to them can converge to coordination protocols that fail to understand and be understood by humans. To demonstrate this, we introduce a simple environment that requires challenging coordination, based on the popular game Overcooked, and learn a simple model that mimics human play. We evaluate the performance of agents trained via self-play and population-based training. These agents perform very well when paired with themselves, but when paired with our human model, they are significantly worse than agents designed to play with the human model. An experiment with a planning algorithm yields the same conclusion, though only when the human-aware planner is given the exact human model that it is playing with. A user study with real humans shows this pattern as well, though less strongly. Qualitatively, we find that the gains come from having the agent adapt to the human's gameplay. Given this result, we suggest several approaches for designing agents that learn about humans in order to better coordinate with them. Code is available at https://github.com/HumanCompatibleAI/overcooked\_ai.},
  langid = {english},
  file = {/Users/yansongli/zotero/storage/LF7V76C9/Carroll et al. - 2019 - On the Utility of Learning about Humans for Human-.pdf;/Users/yansongli/zotero/storage/IPUU7YF6/1910.html}
}

@misc{dong_q-learning_2019,
  title = {Q-Learning with {{UCB Exploration}} Is {{Sample Efficient}} for {{Infinite-Horizon MDP}}},
  author = {Dong, Kefan and Wang, Yuanhao and Chen, Xiaoyu and Wang, Liwei},
  year = {2019},
  month = sep,
  number = {arXiv:1901.09311},
  eprint = {1901.09311},
  primaryclass = {cs, stat},
  publisher = {{arXiv}},
  urldate = {2023-06-19},
  abstract = {A fundamental question in reinforcement learning is whether model-free algorithms are sample efficient. Recently, Jin et al. \textbackslash cite\{jin2018q\} proposed a Q-learning algorithm with UCB exploration policy, and proved it has nearly optimal regret bound for finite-horizon episodic MDP. In this paper, we adapt Q-learning with UCB-exploration bonus to infinite-horizon MDP with discounted rewards \textbackslash emph\{without\} accessing a generative model. We show that the \textbackslash textit\{sample complexity of exploration\} of our algorithm is bounded by \$\textbackslash tilde\{O\}(\{\textbackslash frac\{SA\}\{\textbackslash epsilon\^2(1-\textbackslash gamma)\^7\}\})\$. This improves the previously best known result of \$\textbackslash tilde\{O\}(\{\textbackslash frac\{SA\}\{\textbackslash epsilon\^4(1-\textbackslash gamma)\^8\}\})\$ in this setting achieved by delayed Q-learning \textbackslash cite\{strehl2006pac\}, and matches the lower bound in terms of \$\textbackslash epsilon\$ as well as \$S\$ and \$A\$ except for logarithmic factors.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/Users/yansongli/zotero/storage/EKJU7637/Dong et al. - 2019 - Q-learning with UCB Exploration is Sample Efficien.pdf;/Users/yansongli/zotero/storage/E4I79C5A/1901.html}
}

@article{gmytrasiewicz_framework_2005,
  title = {A {{Framework}} for {{Sequential Planning}} in {{Multi-Agent Settings}}},
  author = {Gmytrasiewicz, P. J. and Doshi, P.},
  year = {2005},
  month = jul,
  journal = {Journal of Artificial Intelligence Research},
  volume = {24},
  pages = {49--79},
  issn = {1076-9757},
  doi = {10.1613/jair.1579},
  urldate = {2023-04-26},
  abstract = {This paper extends the framework of partially observable Markov decision processes (POMDPs) to multi-agent settings by incorporating the notion of agent models into the state space.  Agents maintain beliefs over physical states of the environment and over models of other agents, and they use Bayesian updates to maintain their beliefs over time. The solutions map belief states to actions. Models of other agents may include their belief states and are related to agent types considered in games of incomplete information.  We express the agents' autonomy by postulating that their models are not directly manipulable or observable by other agents.  We show that important properties of POMDPs, such as convergence of value iteration, the rate of convergence, and piece-wise linearity and convexity of the value functions carry over to our framework.  Our approach complements a more traditional approach to interactive settings which uses Nash equilibria as a solution paradigm.  We seek to avoid some of the drawbacks of equilibria which may be non-unique and do not capture off-equilibrium behaviors.  We do so at the cost of having to represent, process and continuously revise models of other agents. Since the agent's beliefs may be arbitrarily nested, the optimal solutions to decision making problems are only asymptotically computable.  However, approximate belief updates and approximately optimal plans are computable. We illustrate our framework using a simple application domain, and we show examples of belief updates and value functions.},
  copyright = {Copyright (c)},
  langid = {english},
  file = {/Users/yansongli/zotero/storage/L7RAKGRF/Gmytrasiewicz and Doshi - 2005 - A Framework for Sequential Planning in Multi-Agent.pdf}
}

@misc{hallak_contextual_2015,
  title = {Contextual {{Markov Decision Processes}}},
  author = {Hallak, Assaf and Di Castro, Dotan and Mannor, Shie},
  year = {2015},
  month = feb,
  number = {arXiv:1502.02259},
  eprint = {1502.02259},
  primaryclass = {cs, stat},
  publisher = {{arXiv}},
  urldate = {2023-05-25},
  abstract = {We consider a planning problem where the dynamics and rewards of the environment depend on a hidden static parameter referred to as the context. The objective is to learn a strategy that maximizes the accumulated reward across all contexts. The new model, called Contextual Markov Decision Process (CMDP), can model a customer's behavior when interacting with a website (the learner). The customer's behavior depends on gender, age, location, device, etc. Based on that behavior, the website objective is to determine customer characteristics, and to optimize the interaction between them. Our work focuses on one basic scenario--finite horizon with a small known number of possible contexts. We suggest a family of algorithms with provable guarantees that learn the underlying models and the latent contexts, and optimize the CMDPs. Bounds are obtained for specific naive implementations, and extensions of the framework are discussed, laying the ground for future research.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/Users/yansongli/zotero/storage/PG4LPUPF/Hallak et al. - 2015 - Contextual Markov Decision Processes.pdf;/Users/yansongli/zotero/storage/LMN6DTBN/1502.html}
}

@inproceedings{han_learning_2018,
  title = {Learning {{Others}}' {{Intentional Models}} in {{Multi-Agent Settings Using Interactive POMDPs}}},
  booktitle = {Advances in {{Neural Information Processing Systems}}},
  author = {Han, Yanlin and Gmytrasiewicz, Piotr},
  year = {2018},
  volume = {31},
  publisher = {{Curran Associates, Inc.}},
  urldate = {2023-04-26},
  abstract = {Interactive partially observable Markov decision processes (I-POMDPs) provide a principled framework for planning and acting in a partially observable, stochastic and multi-agent environment. It extends POMDPs to multi-agent settings by including models of other agents in the state space and forming a hierarchical belief structure. In order to predict other agents' actions using I-POMDPs, we propose an approach that effectively uses Bayesian inference and sequential Monte Carlo sampling to learn others' intentional models which ascribe to them beliefs, preferences and rationality in action selection. Empirical results show that our algorithm accurately learns models of the other agent and has superior performance than methods that use subintentional models. Our approach serves as a generalized Bayesian learning algorithm that learns other agents' beliefs, strategy levels, and transition, observation and reward functions. It also effectively mitigates the belief space complexity due to the nested belief hierarchy.},
  file = {/Users/yansongli/zotero/storage/23KUA24Q/Han and Gmytrasiewicz - 2018 - Learning Others' Intentional Models in Multi-Agent.pdf}
}

@misc{he_opponent_2016,
  title = {Opponent {{Modeling}} in {{Deep Reinforcement Learning}}},
  author = {He, He and {Boyd-Graber}, Jordan and Kwok, Kevin and Daum{\'e} III, Hal},
  year = {2016},
  month = sep,
  number = {arXiv:1609.05559},
  eprint = {1609.05559},
  primaryclass = {cs},
  publisher = {{arXiv}},
  urldate = {2023-04-17},
  abstract = {Opponent modeling is necessary in multi-agent settings where secondary agents with competing goals also adapt their strategies, yet it remains challenging because strategies interact with each other and change. Most previous work focuses on developing probabilistic models or parameterized strategies for specific applications. Inspired by the recent success of deep reinforcement learning, we present neural-based models that jointly learn a policy and the behavior of opponents. Instead of explicitly predicting the opponent's action, we encode observation of the opponents into a deep Q-Network (DQN); however, we retain explicit modeling (if desired) using multitasking. By using a Mixture-of-Experts architecture, our model automatically discovers different strategy patterns of opponents without extra supervision. We evaluate our models on a simulated soccer game and a popular trivia game, showing superior performance over DQN and its variants.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Machine Learning},
  file = {/Users/yansongli/zotero/storage/8BKXDBD8/He et al. - 2016 - Opponent Modeling in Deep Reinforcement Learning.pdf;/Users/yansongli/zotero/storage/JH65LRS9/1609.html}
}

@misc{jiang_contextual_2016,
  title = {Contextual {{Decision Processes}} with {{Low Bellman Rank}} Are {{PAC-Learnable}}},
  author = {Jiang, Nan and Krishnamurthy, Akshay and Agarwal, Alekh and Langford, John and Schapire, Robert E.},
  year = {2016},
  month = dec,
  number = {arXiv:1610.09512},
  eprint = {1610.09512},
  primaryclass = {cs, stat},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.1610.09512},
  urldate = {2023-05-25},
  abstract = {This paper studies systematic exploration for reinforcement learning with rich observations and function approximation. We introduce a new model called contextual decision processes, that unifies and generalizes most prior settings. Our first contribution is a complexity measure, the Bellman rank, that we show enables tractable learning of near-optimal behavior in these processes and is naturally small for many well-studied reinforcement learning settings. Our second contribution is a new reinforcement learning algorithm that engages in systematic exploration to learn contextual decision processes with low Bellman rank. Our algorithm provably learns near-optimal behavior with a number of samples that is polynomial in all relevant parameters but independent of the number of unique observations. The approach uses Bellman error minimization with optimistic exploration and provides new insights into efficient exploration for reinforcement learning with function approximation.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/Users/yansongli/zotero/storage/GNVM7G9J/Jiang et al. - 2016 - Contextual Decision Processes with Low Bellman Ran.pdf;/Users/yansongli/zotero/storage/AFTMSML9/1610.html}
}

@misc{jin_is_2018,
  title = {Is {{Q-learning Provably Efficient}}?},
  author = {Jin, Chi and {Allen-Zhu}, Zeyuan and Bubeck, Sebastien and Jordan, Michael I.},
  year = {2018},
  month = jul,
  number = {arXiv:1807.03765},
  eprint = {1807.03765},
  primaryclass = {cs, math, stat},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.1807.03765},
  urldate = {2023-06-28},
  abstract = {Model-free reinforcement learning (RL) algorithms, such as Q-learning, directly parameterize and update value functions or policies without explicitly modeling the environment. They are typically simpler, more flexible to use, and thus more prevalent in modern deep RL than model-based approaches. However, empirical work has suggested that model-free algorithms may require more samples to learn [Deisenroth and Rasmussen 2011, Schulman et al. 2015]. The theoretical question of "whether model-free algorithms can be made sample efficient" is one of the most fundamental questions in RL, and remains unsolved even in the basic scenario with finitely many states and actions. We prove that, in an episodic MDP setting, Q-learning with UCB exploration achieves regret \$\textbackslash tilde\{O\}(\textbackslash sqrt\{H\^3 SAT\})\$, where \$S\$ and \$A\$ are the numbers of states and actions, \$H\$ is the number of steps per episode, and \$T\$ is the total number of steps. This sample efficiency matches the optimal regret that can be achieved by any model-based approach, up to a single \$\textbackslash sqrt\{H\}\$ factor. To the best of our knowledge, this is the first analysis in the model-free setting that establishes \$\textbackslash sqrt\{T\}\$ regret without requiring access to a "simulator."},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning,Mathematics - Optimization and Control,Statistics - Machine Learning},
  file = {/Users/yansongli/zotero/storage/JLC4U25H/Jin et al. - 2018 - Is Q-learning Provably Efficient.pdf;/Users/yansongli/zotero/storage/QJHTXYU9/1807.html}
}

@misc{kwon_rl_2021,
  title = {{{RL}} for {{Latent MDPs}}: {{Regret Guarantees}} and a {{Lower Bound}}},
  shorttitle = {{{RL}} for {{Latent MDPs}}},
  author = {Kwon, Jeongyeol and Efroni, Yonathan and Caramanis, Constantine and Mannor, Shie},
  year = {2021},
  month = feb,
  number = {arXiv:2102.04939},
  eprint = {2102.04939},
  primaryclass = {cs},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.2102.04939},
  urldate = {2023-05-25},
  abstract = {In this work, we consider the regret minimization problem for reinforcement learning in latent Markov Decision Processes (LMDP). In an LMDP, an MDP is randomly drawn from a set of \$M\$ possible MDPs at the beginning of the interaction, but the identity of the chosen MDP is not revealed to the agent. We first show that a general instance of LMDPs requires at least \$\textbackslash Omega((SA)\^M)\$ episodes to even approximate the optimal policy. Then, we consider sufficient assumptions under which learning good policies requires polynomial number of episodes. We show that the key link is a notion of separation between the MDP system dynamics. With sufficient separation, we provide an efficient algorithm with local guarantee, \{\textbackslash it i.e.,\} providing a sublinear regret guarantee when we are given a good initialization. Finally, if we are given standard statistical sufficiency assumptions common in the Predictive State Representation (PSR) literature (e.g., Boots et al.) and a reachability assumption, we show that the need for initialization can be removed.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Machine Learning},
  file = {/Users/yansongli/zotero/storage/2JVXSFYK/Kwon et al. - 2021 - RL for Latent MDPs Regret Guarantees and a Lower .pdf;/Users/yansongli/zotero/storage/H3PMFVV6/2102.html}
}

@misc{liu_one_2023,
  title = {One {{Objective}} to {{Rule Them All}}: {{A Maximization Objective Fusing Estimation}} and {{Planning}} for {{Exploration}}},
  shorttitle = {One {{Objective}} to {{Rule Them All}}},
  author = {Liu, Zhihan and Lu, Miao and Xiong, Wei and Zhong, Han and Hu, Hao and Zhang, Shenao and Zheng, Sirui and Yang, Zhuoran and Wang, Zhaoran},
  year = {2023},
  month = may,
  number = {arXiv:2305.18258},
  eprint = {2305.18258},
  primaryclass = {cs, math, stat},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.2305.18258},
  urldate = {2023-06-03},
  abstract = {In online reinforcement learning (online RL), balancing exploration and exploitation is crucial for finding an optimal policy in a sample-efficient way. To achieve this, existing sample-efficient online RL algorithms typically consist of three components: estimation, planning, and exploration. However, in order to cope with general function approximators, most of them involve impractical algorithmic components to incentivize exploration, such as optimization within data-dependent level-sets or complicated sampling procedures. To address this challenge, we propose an easy-to-implement RL framework called \textbackslash textit\{Maximize to Explore\} (\textbackslash texttt\{MEX\}), which only needs to optimize \textbackslash emph\{unconstrainedly\} a single objective that integrates the estimation and planning components while balancing exploration and exploitation automatically. Theoretically, we prove that \textbackslash texttt\{MEX\} achieves a sublinear regret with general function approximations for Markov decision processes (MDP) and is further extendable to two-player zero-sum Markov games (MG). Meanwhile, we adapt deep RL baselines to design practical versions of \textbackslash texttt\{MEX\}, in both model-free and model-based manners, which can outperform baselines by a stable margin in various MuJoCo environments with sparse rewards. Compared with existing sample-efficient online RL algorithms with general function approximations, \textbackslash texttt\{MEX\} achieves similar sample efficiency while enjoying a lower computational cost and is more compatible with modern deep RL methods.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computer Science and Game Theory,Computer Science - Machine Learning,Mathematics - Optimization and Control,Statistics - Machine Learning},
  file = {/Users/yansongli/zotero/storage/DPZN5XT7/Liu et al. - 2023 - One Objective to Rule Them All A Maximization Obj.pdf;/Users/yansongli/zotero/storage/U3IZXPTD/2305.html}
}

@misc{xie_learning_2020,
  title = {Learning {{Latent Representations}} to {{Influence Multi-Agent Interaction}}},
  author = {Xie, Annie and Losey, Dylan P. and Tolsma, Ryan and Finn, Chelsea and Sadigh, Dorsa},
  year = {2020},
  month = nov,
  number = {arXiv:2011.06619},
  eprint = {2011.06619},
  primaryclass = {cs},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.2011.06619},
  urldate = {2023-04-30},
  abstract = {Seamlessly interacting with humans or robots is hard because these agents are non-stationary. They update their policy in response to the ego agent's behavior, and the ego agent must anticipate these changes to co-adapt. Inspired by humans, we recognize that robots do not need to explicitly model every low-level action another agent will make; instead, we can capture the latent strategy of other agents through high-level representations. We propose a reinforcement learning-based framework for learning latent representations of an agent's policy, where the ego agent identifies the relationship between its behavior and the other agent's future strategy. The ego agent then leverages these latent dynamics to influence the other agent, purposely guiding them towards policies suitable for co-adaptation. Across several simulated domains and a real-world air hockey game, our approach outperforms the alternatives and learns to influence the other agent.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning,Computer Science - Robotics},
  file = {/Users/yansongli/zotero/storage/3B4G55AN/Xie et al. - 2020 - Learning Latent Representations to Influence Multi.pdf;/Users/yansongli/zotero/storage/87Y2XMQI/2011.html}
}

@misc{zhong_gec_2023,
  title = {{{GEC}}: {{A Unified Framework}} for {{Interactive Decision Making}} in {{MDP}}, {{POMDP}}, and {{Beyond}}},
  shorttitle = {{{GEC}}},
  author = {Zhong, Han and Xiong, Wei and Zheng, Sirui and Wang, Liwei and Wang, Zhaoran and Yang, Zhuoran and Zhang, Tong},
  year = {2023},
  month = jun,
  number = {arXiv:2211.01962},
  eprint = {2211.01962},
  primaryclass = {cs, math, stat},
  publisher = {{arXiv}},
  urldate = {2023-07-18},
  abstract = {We study sample efficient reinforcement learning (RL) under the general framework of interactive decision making, which includes Markov decision process (MDP), partially observable Markov decision process (POMDP), and predictive state representation (PSR) as special cases. Toward finding the minimum assumption that empowers sample efficient learning, we propose a novel complexity measure, generalized eluder coefficient (GEC), which characterizes the fundamental tradeoff between exploration and exploitation in online interactive decision making. In specific, GEC captures the hardness of exploration by comparing the error of predicting the performance of the updated policy with the in-sample training error evaluated on the historical data. We show that RL problems with low GEC form a remarkably rich class, which subsumes low Bellman eluder dimension problems, bilinear class, low witness rank problems, PO-bilinear class, and generalized regular PSR, where generalized regular PSR, a new tractable PSR class identified by us, includes nearly all known tractable POMDPs and PSRs. Furthermore, in terms of algorithm design, we propose a generic posterior sampling algorithm, which can be implemented in both model-free and model-based fashion, under both fully observable and partially observable settings. The proposed algorithm modifies the standard posterior sampling algorithm in two aspects: (i) we use an optimistic prior distribution that biases towards hypotheses with higher values and (ii) a loglikelihood function is set to be the empirical loss evaluated on the historical data, where the choice of loss function supports both model-free and model-based learning. We prove that the proposed algorithm is sample efficient by establishing a sublinear regret upper bound in terms of GEC. In summary, we provide a new and unified understanding of both fully observable and partially observable RL.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning,Mathematics - Optimization and Control,Statistics - Machine Learning},
  file = {/Users/yansongli/zotero/storage/YAA92JK7/Zhong et al. - 2023 - GEC A Unified Framework for Interactive Decision .pdf;/Users/yansongli/zotero/storage/VU9PY7T9/2211.html}
}
