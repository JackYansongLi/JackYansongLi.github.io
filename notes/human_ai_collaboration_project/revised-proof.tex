\documentclass{article}
\usepackage[english]{babel}
\usepackage{amsmath,amssymb,latexsym}

%%%%%%%%%% Start TeXmacs macros
\newcommand{\nin}{\not\in}
\newcommand{\tmop}[1]{\ensuremath{\operatorname{#1}}}
\newenvironment{proof}{\noindent\textbf{Proof\ }}{\hspace*{\fill}$\Box$\medskip}
\newtheorem{lemma}{Lemma}
\newtheorem{theorem}{Theorem}
%%%%%%%%%% End TeXmacs macros

\begin{document}

\begin{lemma}
  \label{lem:loss-upperbound}Let $\Pi$ be a given set of policies, with
  probability at least $1 - \delta$, for any $(h, k) \in [H] \times [K]$, and
  $\pi \in \Pi$,
  \[ L_h^{k - 1} (\pi^{\ast}) - L_h^{k - 1} (\pi) \leq - 2 \sum_{s = 1}^{k -
     1} \mathbb{E}_{\xi_h \sim \mu^s} [\ell_{\pi^s} (\pi ; \xi_h)] + 2 \log (H
     | \Pi | / \delta), \]
  where $\mu^s \in \tmop{BR} (\pi^s)$ for any $\pi^1, \pi^2, \ldots, \pi^k$.
\end{lemma}

\begin{proof}
  Given $\pi \in \Pi$, we denote the random variable $X^k_{h, \pi}$ as
  \[ X^k_{h, \pi} = \log \left( \frac{\mathbb{P}  (s_{h + 1}^k \mid s_h^k,
     a_h^k, \pi^{\ast} (s^k_h))}{\mathbb{P} (s_{h + 1}^k \mid s_h^k, a_h^k,
     \pi (s^k_h))} \right) . \]
  Now we define a filtration $\{ \mathcal{F}_{h, k} \}_{k = 1}^K$ as (B.25) in
  {\cite{liu_one_2023}}. Thus we have $X_{h, \pi}^k \in \mathcal{F}_{h, k}$.
  Therefore, by applying Lemma D.1 in {\cite{liu_one_2023}}, we have that with
  probability at least $1 - \delta$, for any $(h, k) \in [H] \times [K]$, and
  $\pi \in \Pi$, we have
  \begin{equation}
    - \frac{1}{2} \sum_{s = 1}^{k - 1} X_{h, \pi}^s \leq \sum_{s = 1}^{k - 1}
    \log \mathbb{E} \left[ \exp \left\{ - \frac{1}{2} X_{h, \pi}^s \right\}
    \mid \mathcal{F}_{h, s - 1} \right] + \log (H | \Pi | / \delta) .
    \label{eq:union-bound}
  \end{equation}
  Meanwhile, by (B.27) in {\cite{liu_one_2023}}, for any $\mu^s \in \tmop{BR}
  (\pi^s)$, the conditional expectation equals to
  \begin{equation}
    \mathbb{E} \left[ \exp \left\{ - \frac{1}{2} X_{h, \pi}^s \right\} \mid
    \mathcal{F}_{h, s - 1} \right] = 1 -\mathbb{E}_{(s_h^s, a_h^s) \sim \mu^s}
    [D_H (\mathbb{P} (\cdot \mid s_h^s, a_h^s, \pi^{\ast} (s_h^s)), \mathbb{P}
    (\cdot \mid s_h^s, a_h^s, \pi (s^s_h)))] . \label{eq:cond-exp}
  \end{equation}
  Denote $\mathbb{E}_{(s_h^s, a_h^s) \sim \mu^s} [D_H (\mathbb{P} (\cdot \mid
  s_h^s, a_h^s, \pi^{\ast} (s_h^s)), \mathbb{P} (\cdot \mid s_h^s, a_h^s, \pi
  (s^s_h)))]$ as $\mathbb{E}_{\xi_h \sim \mu^s} [\ell_{\pi^s} (\pi ; \xi_h)]$.
  Using the fact $\log (x) \leq x - 1$ and substituting \eqref{eq:cond-exp}
  into \eqref{eq:union-bound} finishes the proof.
\end{proof}

All guessed partner's policies in $K$ episodes forms a set $\{ \pi^i \}_{i =
1}^K$, let $\mathcal{H}_{\psi}$ be a subset of $\{ \pi^i \}_{i = 1}^K$
satisfying the following preperty: $\pi^l$ is in $\mathcal{H}_{\psi}$ if and
only if there does not exist $k > l$ such that $\pi^k \overset{\psi}{\sim}
\pi^l$. We have the following lemma.

\begin{lemma}
  \label{lem:increasing-seq}If for all $k \in [K]$ such that $\pi^k \in
  \mathcal{H}_{\psi}$, we have
  \[ V \left( \psi (\pi^{\ast}) {, \pi^{\ast}}^{} \right) - V (\psi (\pi^k),
     \pi^k) \leq c_k, \]
  where $\{ c_k \}_{k \in [K]}$ is a non-increasing sequence. Then, for all $k
  \in [K]$, we have
  \[ V \left( \psi (\pi^{\ast}) {, \pi^{\ast}}^{} \right) - V (\psi (\pi^k),
     \pi^k) \leq c_k . \]
\end{lemma}

\begin{proof}
  By definition, for all $k$, $l \in [K]$ with $k > l$ and $\pi^k
  \overset{\psi}{\sim} \pi^l$, we have
  \[ V (\psi (\pi^k), \pi^k) = V (\psi (\pi^l), \pi^l) . \]
  Thus,
  \[ V \left( \psi (\pi^{\ast}) {, \pi^{\ast}}^{} \right) - V (\psi (\pi^k),
     \pi^k) = V \left( \psi (\pi^{\ast}) {, \pi^{\ast}}^{} \right) - V (\psi
     (\pi^l), \pi^l) . \]
  Note that for all $k \in [K]$ such that $\pi^k \in \mathcal{H}_{\psi}$, we
  have
  \[ V \left( \psi (\pi^{\ast}) {, \pi^{\ast}}^{} \right) - V (\psi (\pi^k),
     \pi^k) \leq c_k, \]
  which implies $V \left( \psi (\pi^{\ast}) {, \pi^{\ast}}^{} \right) - V
  (\psi (\pi^l), \pi^l) \leq c_k$. By the construction rule of
  $\mathcal{H}_{\psi}$, for all $l \in [K]$ with $\pi^l \nin
  \mathcal{H}_{\psi}$, we can always find a constant $k'$ such that $k' > l$
  and $\pi^{k'} \in \mathcal{H}_{\psi}$. Thus
  \[ V (\psi (\pi^{\ast}), \pi^{\ast}) - V (\psi (\pi^l), \pi^l) \leq c_{k'}
     \leq c_l . \]
\end{proof}

Based on this lemma, we can prove that the regret upper bound given a finite
hypothsis set depends on the type number of the hypothesis set rather than the
size of the hypothesis set.

\begin{theorem}
  \label{thm:fin}Given an MDP with generalized eluder coefficient
  $d_{\tmop{GEC}} (\cdot)$ and a finite hypothesis class
  $\mathcal{H}_{\tmop{fin}} $ with $\pi^{\ast} \in \mathcal{H}_{\tmop{fin}}$,
  by setting
  \[ \eta = \sqrt{\frac{d_{\tmop{GEC}} \left( 1 / \sqrt{H K} \right)}{\log (H
     n^{\psi} (\mathcal{H}_{\tmop{fin}}) / \delta) \cdot H K}}, \]
  the regret of the MEX algorithm applying on $\mathcal{H}_{\tmop{fin}}$ with
  oracle $\psi$ after $K$ episodes is upper bounded by, with probability at
  least $1 - \delta$,
  \[ \tmop{Regret} (K) \lesssim \sqrt{d_{\tmop{GEC}} \left( 1 / \sqrt{H K}
     \right) \cdot \log (H n^{\psi} (\mathcal{H}_{\tmop{fin}}) / \delta) \cdot
     H K} . \]
\end{theorem}

\begin{proof}
  We decompose the regret into two terms,
  
  \begin{align}
    \tmop{Regret} (K) \triangleq & {\sum_{k = 1}^K}  V \left( \psi
    (\pi^{\ast}) {, \pi^{\ast}}^{} \right) - V (\psi (\pi^k), \pi^{\ast})
    \nonumber\\
    = & \underbrace{\sum_{k = 1}^K V \left( \psi (\pi^{\ast}) {,
    \pi^{\ast}}^{} \right) - V (\psi (\pi^k), \pi^k)}_{\text{Term (i)} } +
    \underbrace{\sum_{k = 1}^K V (\psi (\pi^k), \pi^k) - V (\psi (\pi^k),
    \pi^{\ast})}_{\text{Term  (ii)}} . \nonumber
  \end{align}
  
  \textbf{Term (i).} By the choice of $\pi^k$, we have
  \[ V (\psi (\pi^{\ast}), \pi^{\ast}) - \eta \sum_{h = 1}^H L_h^{k - 1}
     (\pi^{\ast}) \leq V (\psi (\pi^k), \pi^k) - \eta \sum_{h = 1}^H L_h^{k -
     1} (\pi^k) \]
  for all $k \in [K]$. Thus,
  \begin{equation}
    V (\psi (\pi^{\ast}), \pi^{\ast}) - V (\psi (\pi^k), \pi^k) \leq \eta
    \sum_{h = 1}^H (L_h^{k - 1} (\pi^{\ast}) - L_h^{k - 1} (\pi^k)) .
    \label{eq:gen-error}
  \end{equation}
  Applying Lemma \ref{lem:loss-upperbound}, we have that with probability at
  least $1 - \delta$, for any $(h, k) \in [H] \times [K]$ and all $\pi \in
  \mathcal{H}_{\psi}$,
  \[ L_h^{k - 1} (\pi^{\ast}) - L_h^{k - 1} (\pi) \leq - 2 \sum_{s = 1}^{k -
     1} \mathbb{E}_{\xi_h \sim \psi (\pi^s)} [\ell_{\pi^s} (\pi ; \xi_h)] + 2
     \log (H | \mathcal{H}_{\psi} | / \delta) . \]
  Substituting the above equation into \eqref{eq:gen-error} gives us, with
  probability at least $1 - \delta$, for all $k \in [K]$ with $\pi^k \in
  \mathcal{H}_{\psi}$, we have
  \[ V (\psi (\pi^{\ast}), \pi^{\ast}) - V (\psi (\pi^k), \pi^k) \leq - 2 \eta
     \sum_{h = 1}^H \sum_{s = 1}^{k - 1} \mathbb{E}_{\xi_h \sim \psi (\pi^s)}
     [\ell_{\pi^s} (\pi^k ; \xi_h)] + 2 H \eta \log (H | \mathcal{H}_{\psi} |
     / \delta) \]
  We define $c_k$ as
  \[ c_k \triangleq - 2 \eta \sum_{h = 1}^H \sum_{s = 1}^{k - 1}
     \mathbb{E}_{\xi_h \sim \psi (\pi^s)} [\ell_{\pi^s} (\pi^k ; \xi_h)] + 2 H
     \eta \log (H | \mathcal{H}_{\psi} | / \delta) . \]
  The sequnce $\{ c_k \}_{k \in [K]}$ is a non-increasing sequence. Applying
  Lemma \ref{lem:increasing-seq} gives us, with probability at least $1 -
  \delta$, for all $k \in [K]$, we have
  \[ V (\psi (\pi^{\ast}), \pi^{\ast}) - V (\psi (\pi^k), \pi^k) \leq c_k .
  \]
  Summing over $[K]$ gives us, with probability $1 - \delta$,
  
  \begin{align}
    \text{Term (i)} \leq & \sum_{k = 1}^K c_k \nonumber\\
    = & - 2 \eta \sum_{k = 1}^K \sum_{h = 1}^H \sum_{s = 1}^{k - 1}
    \mathbb{E}_{\xi_h \sim \psi (\pi^s)} [\ell_{\pi^s} (\pi^k ; \xi_h)] + 2 H
    K \eta \log (H | \mathcal{H}_{\psi} | / \delta) \nonumber\\
    \leq & - 2 \eta \sum_{k = 1}^K \sum_{h = 1}^H \sum_{s = 1}^{k - 1}
    \mathbb{E}_{\xi_h \sim \psi (\pi^s)} [\ell_{\pi^s} (\pi^k ; \xi_h)] + 2 H
    K \eta \log (H n_{\psi} (\mathcal{H}_{\tmop{fin}}) / \delta) . \nonumber
  \end{align}
  
  \
  
  \textbf{Term (ii).} Follow the proof of Theorem 4.4 in
  {\cite{liu_one_2023}},
  \[ \tmop{Term} (\tmop{ii}) \leq 2 \eta \sum_{k = 1}^K \sum_{h = 1}^H \sum_{s
     = 1}^{k - 1} \mathbb{E}_{\xi_h \sim \psi (\pi^s)} [\ell_{\pi^s} (\pi^k ;
     \xi_h)] + \frac{d_{\tmop{GEC}} (\varepsilon_{\tmop{conf}})}{8 \eta} +
     \sqrt{d_{\tmop{GEC}} (\varepsilon_{\tmop{conf}}) H K} +
     \varepsilon_{\tmop{conf}} H K. \]
  \textbf{Combining Term (i) and Term (ii).}
  
  \begin{align}
    \tmop{Regret} (K) = & \text{Term (i)} + \text{Term (ii)} \nonumber\\
    \leq & 2 \eta K H \log (H n^{\psi} (\mathcal{H}_{\tmop{fin}}) / \delta) +
    \frac{d_{\tmop{GEC}} (\varepsilon_{\tmop{conf}})}{8 \eta} +
    \sqrt{d_{\tmop{GEC}} (\varepsilon_{\tmop{conf}}) H K} +
    \varepsilon_{\tmop{conf}} H K. \nonumber
  \end{align}
  
  Set $\varepsilon_{\tmop{conf}} = 1 / \sqrt{H K}$ and
  \[ \eta = \sqrt{\frac{d_{\tmop{GEC}} \left( 1 / \sqrt{H K} \right)}{\log (H
     n^{\psi} (\mathcal{H}_{\tmop{fin}}) / \delta) \cdot H K}} \]
  leads to the proof.
\end{proof}

\begin{thebibliography}{1}
  \bibitem[1]{liu_one_2023}Zhihan Liu, Miao Lu, Wei Xiong, Han Zhong, Hao Hu,
  Shenao Zhang, Sirui Zheng, Zhuoran Yang, and  Zhaoran Wang. {\newblock}One
  Objective to Rule Them All: A Maximization Objective Fusing Estimation and
  Planning for Exploration. {\newblock}may 2023.{\newblock}
\end{thebibliography}

\end{document}
