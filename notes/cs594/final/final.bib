@misc{kwon_rl_2021,
  title = {{{RL}} for {{Latent MDPs}}: {{Regret Guarantees}} and a {{Lower Bound}}},
  shorttitle = {{{RL}} for {{Latent MDPs}}},
  author = {Kwon, Jeongyeol and Efroni, Yonathan and Caramanis, Constantine and Mannor, Shie},
  year = {2021},
  month = feb,
  number = {arXiv:2102.04939},
  eprint = {2102.04939},
  primaryclass = {cs},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.2102.04939},
  urldate = {2023-03-23},
  abstract = {In this work, we consider the regret minimization problem for reinforcement learning in latent Markov Decision Processes (LMDP). In an LMDP, an MDP is randomly drawn from a set of \$M\$ possible MDPs at the beginning of the interaction, but the identity of the chosen MDP is not revealed to the agent. We first show that a general instance of LMDPs requires at least \$\textbackslash Omega((SA)\^M)\$ episodes to even approximate the optimal policy. Then, we consider sufficient assumptions under which learning good policies requires polynomial number of episodes. We show that the key link is a notion of separation between the MDP system dynamics. With sufficient separation, we provide an efficient algorithm with local guarantee, \{\textbackslash it i.e.,\} providing a sublinear regret guarantee when we are given a good initialization. Finally, if we are given standard statistical sufficiency assumptions common in the Predictive State Representation (PSR) literature (e.g., Boots et al.) and a reachability assumption, we show that the need for initialization can be removed.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Machine Learning},
  file = {/Users/yansongli/zotero/storage/C72Z2VAG/Kwon et al. - 2021 - RL for Latent MDPs Regret Guarantees and a Lower .pdf;/Users/yansongli/zotero/storage/YNG7G75B/2102.html}
}

@book{meyn_control_2022,
  title = {Control {{Systems}} and {{Reinforcement Learning}}},
  author = {Meyn, Sean},
  year = {2022},
  month = may,
  edition = {First},
  publisher = {{Cambridge University Press}},
  doi = {10.1017/9781009051873},
  urldate = {2023-01-23},
  abstract = {A high school student can create deep Q-learning code to control her robot, without any understanding of the meaning of 'deep' or 'Q', or why the code sometimes fails. This book is designed to explain the science behind reinforcement learning and optimal control in a way that is accessible to students with a background in calculus and matrix algebra. A unique focus is algorithm design to obtain the fastest possible speed of convergence for learning algorithms, along with insight into why reinforcement learning sometimes fails. Advanced stochastic process theory is avoided at the start by substituting random exploration with more intuitive deterministic probing for learning. Once these ideas are understood, it is not difficult to master techniques rooted in stochastic control. These topics are covered in the second part of the book, starting with Markov chain theory and ending with a fresh look at actor-critic methods for reinforcement learning.},
  isbn = {978-1-00-905187-3 978-1-316-51196-1},
  file = {/Users/yansongli/zotero/storage/PEVGQHVC/CSRLonline.pdf}
}

@article{nikolaidis_human-robot_2017,
  title = {Human-Robot Mutual Adaptation in Collaborative Tasks: {{Models}} and Experiments},
  shorttitle = {Human-Robot Mutual Adaptation in Collaborative Tasks},
  author = {Nikolaidis, Stefanos and Hsu, David and Srinivasa, Siddhartha},
  year = {2017},
  month = jun,
  journal = {The International Journal of Robotics Research},
  volume = {36},
  number = {5-7},
  pages = {618--634},
  publisher = {{SAGE Publications Ltd STM}},
  issn = {0278-3649},
  doi = {10.1177/0278364917690593},
  urldate = {2023-03-25},
  abstract = {Adaptation is critical for effective team collaboration. This paper introduces a computational formalism for mutual adaptation between a robot and a human in collaborative tasks. We propose the Bounded-Memory Adaptation Model, which is a probabilistic finite-state controller that captures human adaptive behaviors under a bounded-memory assumption. We integrate the Bounded-Memory Adaptation Model into a probabilistic decision process, enabling the robot to guide adaptable participants towards a better way of completing the task. Human subject experiments suggest that the proposed formalism improves the effectiveness of human-robot teams in collaborative tasks, when compared with one-way adaptations of the robot to the human, while maintaining the human?s trust in the robot.},
  file = {/Users/yansongli/zotero/storage/B5YSNEK7/Nikolaidis et al. - 2017 - Human-robot mutual adaptation in collaborative tas.pdf}
}

@misc{yang_overview_2021,
  title = {An {{Overview}} of {{Multi-Agent Reinforcement Learning}} from {{Game Theoretical Perspective}}},
  author = {Yang, Yaodong and Wang, Jun},
  year = {2021},
  month = mar,
  number = {arXiv:2011.00583},
  eprint = {2011.00583},
  primaryclass = {cs},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.2011.00583},
  urldate = {2022-10-19},
  abstract = {Following the remarkable success of the AlphaGO series, 2019 was a booming year that witnessed significant advances in multi-agent reinforcement learning (MARL) techniques. MARL corresponds to the learning problem in a multi-agent system in which multiple agents learn simultaneously. It is an interdisciplinary domain with a long history that includes game theory, machine learning, stochastic control, psychology, and optimisation. Although MARL has achieved considerable empirical success in solving real-world games, there is a lack of a self-contained overview in the literature that elaborates the game theoretical foundations of modern MARL methods and summarises the recent advances. In fact, the majority of existing surveys are outdated and do not fully cover the recent developments since 2010. In this work, we provide a monograph on MARL that covers both the fundamentals and the latest developments in the research frontier. The goal of our monograph is to provide a self-contained assessment of the current state-of-the-art MARL techniques from a game theoretical perspective. We expect this work to serve as a stepping stone for both new researchers who are about to enter this fast-growing domain and existing domain experts who want to obtain a panoramic view and identify new directions based on recent advances.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Multiagent Systems},
  file = {/Users/yansongli/zotero/storage/JT3H3JWZ/Yang and Wang - 2021 - An Overview of Multi-Agent Reinforcement Learning .pdf;/Users/yansongli/zotero/storage/4XR4LL49/2011.html}
}

@misc{zhang_multi-agent_2021,
  title = {Multi-{{Agent Reinforcement Learning}}: {{A Selective Overview}} of {{Theories}} and {{Algorithms}}},
  shorttitle = {Multi-{{Agent Reinforcement Learning}}},
  author = {Zhang, Kaiqing and Yang, Zhuoran and Ba{\c s}ar, Tamer},
  year = {2021},
  month = apr,
  number = {arXiv:1911.10635},
  eprint = {1911.10635},
  primaryclass = {cs, stat},
  publisher = {{arXiv}},
  urldate = {2023-03-20},
  abstract = {Recent years have witnessed significant advances in reinforcement learning (RL), which has registered great success in solving various sequential decision-making problems in machine learning. Most of the successful RL applications, e.g., the games of Go and Poker, robotics, and autonomous driving, involve the participation of more than one single agent, which naturally fall into the realm of multi-agent RL (MARL), a domain with a relatively long history, and has recently re-emerged due to advances in single-agent RL techniques. Though empirically successful, theoretical foundations for MARL are relatively lacking in the literature. In this chapter, we provide a selective overview of MARL, with focus on algorithms backed by theoretical analysis. More specifically, we review the theoretical results of MARL algorithms mainly within two representative frameworks, Markov/stochastic games and extensive-form games, in accordance with the types of tasks they address, i.e., fully cooperative, fully competitive, and a mix of the two. We also introduce several significant but challenging applications of these algorithms. Orthogonal to the existing reviews on MARL, we highlight several new angles and taxonomies of MARL theory, including learning in extensive-form games, decentralized MARL with networked agents, MARL in the mean-field regime, (non-)convergence of policy-based methods for learning in games, etc. Some of the new angles extrapolate from our own research endeavors and interests. Our overall goal with this chapter is, beyond providing an assessment of the current state of the field on the mark, to identify fruitful future research directions on theoretical studies of MARL. We expect this chapter to serve as continuing stimulus for researchers interested in working on this exciting while challenging topic.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning,Computer Science - Multiagent Systems,Statistics - Machine Learning},
  file = {/Users/yansongli/zotero/storage/PK3LBG7U/Zhang et al. - 2021 - Multi-Agent Reinforcement Learning A Selective Ov.pdf;/Users/yansongli/zotero/storage/NQIS7QGR/1911.html}
}
