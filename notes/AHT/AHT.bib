@article{albrecht_belief_2016,
  title = {Belief and {{Truth}} in {{Hypothesised Behaviours}}},
  author = {Albrecht, Stefano V. and Crandall, Jacob W. and Ramamoorthy, Subramanian},
  year = {2016},
  month = jun,
  journal = {Artificial Intelligence},
  volume = {235},
  eprint = {1507.07688},
  primaryclass = {cs},
  pages = {63--94},
  issn = {00043702},
  doi = {10.1016/j.artint.2016.02.004},
  urldate = {2023-04-30},
  abstract = {There is a long history in game theory on the topic of Bayesian or "rational" learning, in which each player maintains beliefs over a set of alternative behaviours, or types, for the other players. This idea has gained increasing interest in the artificial intelligence (AI) community, where it is used as a method to control a single agent in a system composed of multiple agents with unknown behaviours. The idea is to hypothesise a set of types, each specifying a possible behaviour for the other agents, and to plan our own actions with respect to those types which we believe are most likely, given the observed actions of the agents. The game theory literature studies this idea primarily in the context of equilibrium attainment. In contrast, many AI applications have a focus on task completion and payoff maximisation. With this perspective in mind, we identify and address a spectrum of questions pertaining to belief and truth in hypothesised types. We formulate three basic ways to incorporate evidence into posterior beliefs and show when the resulting beliefs are correct, and when they may fail to be correct. Moreover, we demonstrate that prior beliefs can have a significant impact on our ability to maximise payoffs in the long-term, and that they can be computed automatically with consistent performance effects. Furthermore, we analyse the conditions under which we are able complete our task optimally, despite inaccuracies in the hypothesised types. Finally, we show how the correctness of hypothesised types can be ascertained during the interaction via an automated statistical analysis.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computer Science and Game Theory,I.2.11},
  file = {/home/jackyansongli/Zotero/storage/PXIQTCBF/Albrecht et al. - 2016 - Belief and Truth in Hypothesised Behaviours.pdf;/home/jackyansongli/Zotero/storage/KEN4KWIM/1507.html}
}

@misc{albrecht_e-hba_2019,
  title = {E-{{HBA}}: {{Using Action Policies}} for {{Expert Advice}} and {{Agent Typification}}},
  shorttitle = {E-{{HBA}}},
  author = {Albrecht, Stefano V. and Crandall, Jacob W. and Ramamoorthy, Subramanian},
  year = {2019},
  month = jul,
  number = {arXiv:1907.09810},
  eprint = {1907.09810},
  primaryclass = {cs},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.1907.09810},
  urldate = {2023-04-30},
  abstract = {Past research has studied two approaches to utilise predefined policy sets in repeated interactions: as experts, to dictate our own actions, and as types, to characterise the behaviour of other agents. In this work, we bring these complementary views together in the form of a novel meta-algorithm, called Expert-HBA (E-HBA), which can be applied to any expert algorithm that considers the average (or total) payoff an expert has yielded in the past. E-HBA gradually mixes the past payoff with a predicted future payoff, which is computed using the type-based characterisation. We present results from a comprehensive set of repeated matrix games, comparing the performance of several well-known expert algorithms with and without the aid of E-HBA. Our results show that E-HBA has the potential to significantly improve the performance of expert algorithms.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Multiagent Systems},
  file = {/home/jackyansongli/Zotero/storage/2DIRZG4V/Albrecht et al. - 2019 - E-HBA Using Action Policies for Expert Advice and.pdf;/home/jackyansongli/Zotero/storage/WL45CNNI/1907.html}
}

@misc{albrecht_empirical_2019,
  title = {An {{Empirical Study}} on the {{Practical Impact}} of {{Prior Beliefs}} over {{Policy Types}}},
  author = {Albrecht, Stefano V. and Crandall, Jacob W. and Ramamoorthy, Subramanian},
  year = {2019},
  month = jul,
  number = {arXiv:1907.05247},
  eprint = {1907.05247},
  primaryclass = {cs},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.1907.05247},
  urldate = {2023-04-30},
  abstract = {Many multiagent applications require an agent to learn quickly how to interact with previously unknown other agents. To address this problem, researchers have studied learning algorithms which compute posterior beliefs over a hypothesised set of policies, based on the observed actions of the other agents. The posterior belief is complemented by the prior belief, which specifies the subjective likelihood of policies before any actions are observed. In this paper, we present the first comprehensive empirical study on the practical impact of prior beliefs over policies in repeated interactions. We show that prior beliefs can have a significant impact on the long-term performance of such methods, and that the magnitude of the impact depends on the depth of the planning horizon. Moreover, our results demonstrate that automatic methods can be used to compute prior beliefs with consistent performance effects. This indicates that prior beliefs could be eliminated as a manual parameter and instead be computed automatically.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Multiagent Systems},
  file = {/home/jackyansongli/Zotero/storage/SC7IAB7K/Albrecht et al. - 2019 - An Empirical Study on the Practical Impact of Prio.pdf;/home/jackyansongli/Zotero/storage/NVMAAEAC/1907.html}
}

@misc{albrecht_game-theoretic_2015,
  title = {A {{Game-Theoretic Model}} and {{Best-Response Learning Method}} for {{Ad Hoc Coordination}} in {{Multiagent Systems}}},
  author = {Albrecht, Stefano V. and Ramamoorthy, Subramanian},
  year = {2015},
  month = jun,
  number = {arXiv:1506.01170},
  eprint = {1506.01170},
  primaryclass = {cs},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.1506.01170},
  urldate = {2023-04-30},
  abstract = {The ad hoc coordination problem is to design an autonomous agent which is able to achieve optimal flexibility and efficiency in a multiagent system with no mechanisms for prior coordination. We conceptualise this problem formally using a game-theoretic model, called the stochastic Bayesian game, in which the behaviour of a player is determined by its private information, or type. Based on this model, we derive a solution, called Harsanyi-Bellman Ad Hoc Coordination (HBA), which utilises the concept of Bayesian Nash equilibrium in a planning procedure to find optimal actions in the sense of Bellman optimal control. We evaluate HBA in a multiagent logistics domain called level-based foraging, showing that it achieves higher flexibility and efficiency than several alternative algorithms. We also report on a human-machine experiment at a public science exhibition in which the human participants played repeated Prisoner's Dilemma and Rock-Paper-Scissors against HBA and alternative algorithms, showing that HBA achieves equal efficiency and a significantly higher welfare and winning rate.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computer Science and Game Theory,Computer Science - Multiagent Systems},
  file = {/home/jackyansongli/Zotero/storage/BKEGCN6F/Albrecht and Ramamoorthy - 2015 - A Game-Theoretic Model and Best-Response Learning .pdf;/home/jackyansongli/Zotero/storage/CYAM46GI/1506.html}
}

@article{barrett_making_2017,
  title = {Making Friends on the Fly: {{Cooperating}} with New Teammates},
  shorttitle = {Making Friends on the Fly},
  author = {Barrett, Samuel and Rosenfeld, Avi and Kraus, Sarit and Stone, Peter},
  year = {2017},
  month = jan,
  journal = {Artificial Intelligence},
  volume = {242},
  pages = {132--171},
  issn = {00043702},
  doi = {10.1016/j.artint.2016.10.005},
  urldate = {2023-04-30},
  langid = {english},
  file = {/home/jackyansongli/Zotero/storage/9R3LU8YF/Barrett et al. - 2017 - Making friends on the fly Cooperating with new te.pdf}
}

@incollection{baumeister_survey_2022,
  title = {A {{Survey}} of {{Ad Hoc Teamwork Research}}},
  booktitle = {Multi-{{Agent Systems}}},
  author = {Mirsky, Reuth and Carlucho, Ignacio and Rahman, Arrasy and Fosong, Elliot and Macke, William and Sridharan, Mohan and Stone, Peter and Albrecht, Stefano V.},
  editor = {Baumeister, Dorothea and Rothe, J{\"o}rg},
  year = {2022},
  volume = {13442},
  pages = {275--293},
  publisher = {{Springer International Publishing}},
  address = {{Cham}},
  doi = {10.1007/978-3-031-20614-6_16},
  urldate = {2023-04-27},
  abstract = {Ad hoc teamwork is the research problem of designing agents that can collaborate with new teammates without prior coordination. This survey makes a two-fold contribution: First, it provides a structured description of the different facets of the ad hoc teamwork problem. Second, it discusses the progress that has been made in the field so far, and identifies the immediate and long-term open problems that need to be addressed in ad hoc teamwork.},
  isbn = {978-3-031-20613-9 978-3-031-20614-6},
  langid = {english},
  file = {/home/jackyansongli/Zotero/storage/9YM8QPDM/Mirsky et al. - 2022 - A Survey of Ad Hoc Teamwork Research.pdf}
}

@misc{noauthor_empirical_nodate,
  title = {Empirical Evaluation of Ad Hoc Teamwork in the Pursuit Domain. - {{Google Search}}},
  urldate = {2023-04-30},
  howpublished = {https://www.google.com/search?q=Empirical+evaluation+of+ad+hoc+teamwork+in+the+pursuit+domain.\&oq=Empirical+evaluation+of+ad+hoc+teamwork+in+the+pursuit+domain.\&aqs=chrome..69i57.367j0j7\&sourceid=chrome\&ie=UTF-8}
}

@misc{rabinowitz_machine_2018,
  title = {Machine {{Theory}} of {{Mind}}},
  author = {Rabinowitz, Neil C. and Perbet, Frank and Song, H. Francis and Zhang, Chiyuan and Eslami, S. M. Ali and Botvinick, Matthew},
  year = {2018},
  month = mar,
  number = {arXiv:1802.07740},
  eprint = {1802.07740},
  primaryclass = {cs},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.1802.07740},
  urldate = {2023-04-30},
  abstract = {Theory of mind (ToM; Premack \& Woodruff, 1978) broadly refers to humans' ability to represent the mental states of others, including their desires, beliefs, and intentions. We propose to train a machine to build such models too. We design a Theory of Mind neural network -- a ToMnet -- which uses meta-learning to build models of the agents it encounters, from observations of their behaviour alone. Through this process, it acquires a strong prior model for agents' behaviour, as well as the ability to bootstrap to richer predictions about agents' characteristics and mental states using only a small number of behavioural observations. We apply the ToMnet to agents behaving in simple gridworld environments, showing that it learns to model random, algorithmic, and deep reinforcement learning agents from varied populations, and that it passes classic ToM tasks such as the "Sally-Anne" test (Wimmer \& Perner, 1983; Baron-Cohen et al., 1985) of recognising that others can hold false beliefs about the world. We argue that this system -- which autonomously learns how to model other agents in its world -- is an important step forward for developing multi-agent AI systems, for building intermediating technology for machine-human interaction, and for advancing the progress on interpretable AI.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Artificial Intelligence},
  file = {/home/jackyansongli/Zotero/storage/4VVCG3JE/Rabinowitz et al. - 2018 - Machine Theory of Mind.pdf;/home/jackyansongli/Zotero/storage/IHEE46LI/1802.html}
}

@inproceedings{ravula_ad_2019,
  title = {Ad {{Hoc Teamwork With Behavior Switching Agents}}},
  booktitle = {Proceedings of the {{Twenty-Eighth International Joint Conference}} on {{Artificial Intelligence}}},
  author = {Ravula, Manish and Alkoby, Shani and Stone, Peter},
  year = {2019},
  month = aug,
  pages = {550--556},
  publisher = {{International Joint Conferences on Artificial Intelligence Organization}},
  address = {{Macao, China}},
  doi = {10.24963/ijcai.2019/78},
  urldate = {2023-04-30},
  abstract = {As autonomous AI agents proliferate in the real world, they will increasingly need to cooperate with each other to achieve complex goals without always being able to coordinate in advance. This kind of cooperation, in which agents have to learn to cooperate on the fly, is called ad hoc teamwork. Many previous works investigating this setting assumed that teammates behave according to one of many predefined types that is fixed throughout the task. This assumption of stationarity in behaviors, is a strong assumption which cannot be guaranteed in many real-world settings. In this work, we relax this assumption and investigate settings in which teammates can change their types during the course of the task. This adds complexity to the planning problem as now an agent needs to recognize that a change has occurred in addition to figuring out what is the new type of the teammate it is interacting with. In this paper, we present a novel Convolutional-Neural-Networkbased Change Point Detection (CPD) algorithm for ad hoc teamwork. When evaluating our algorithm on the modified predator prey domain, we find that it outperforms existing Bayesian CPD algorithms.},
  isbn = {978-0-9992411-4-1},
  langid = {english},
  file = {/home/jackyansongli/Zotero/storage/HFKYLKYM/Ravula et al. - 2019 - Ad Hoc Teamwork With Behavior Switching Agents.pdf}
}

@misc{xie_learning_2020,
  title = {Learning {{Latent Representations}} to {{Influence Multi-Agent Interaction}}},
  author = {Xie, Annie and Losey, Dylan P. and Tolsma, Ryan and Finn, Chelsea and Sadigh, Dorsa},
  year = {2020},
  month = nov,
  number = {arXiv:2011.06619},
  eprint = {2011.06619},
  primaryclass = {cs},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.2011.06619},
  urldate = {2023-04-30},
  abstract = {Seamlessly interacting with humans or robots is hard because these agents are non-stationary. They update their policy in response to the ego agent's behavior, and the ego agent must anticipate these changes to co-adapt. Inspired by humans, we recognize that robots do not need to explicitly model every low-level action another agent will make; instead, we can capture the latent strategy of other agents through high-level representations. We propose a reinforcement learning-based framework for learning latent representations of an agent's policy, where the ego agent identifies the relationship between its behavior and the other agent's future strategy. The ego agent then leverages these latent dynamics to influence the other agent, purposely guiding them towards policies suitable for co-adaptation. Across several simulated domains and a real-world air hockey game, our approach outperforms the alternatives and learns to influence the other agent.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning,Computer Science - Robotics},
  file = {/home/jackyansongli/Zotero/storage/3B4G55AN/Xie et al. - 2020 - Learning Latent Representations to Influence Multi.pdf;/home/jackyansongli/Zotero/storage/87Y2XMQI/2011.html}
}

@misc{zintgraf_deep_2022,
  title = {Deep {{Interactive Bayesian Reinforcement Learning}} via {{Meta-Learning}}},
  author = {Zintgraf, Luisa and Devlin, Sam and Ciosek, Kamil and Whiteson, Shimon and Hofmann, Katja},
  year = {2022},
  month = apr,
  number = {arXiv:2101.03864},
  eprint = {2101.03864},
  primaryclass = {cs},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.2101.03864},
  urldate = {2023-04-30},
  abstract = {Agents that interact with other agents often do not know a priori what the other agents' strategies are, but have to maximise their own online return while interacting with and learning about others. The optimal adaptive behaviour under uncertainty over the other agents' strategies w.r.t. some prior can in principle be computed using the Interactive Bayesian Reinforcement Learning framework. Unfortunately, doing so is intractable in most settings, and existing approximation methods are restricted to small tasks. To overcome this, we propose to meta-learn approximate belief inference and Bayes-optimal behaviour for a given prior. To model beliefs over other agents, we combine sequential and hierarchical Variational Auto-Encoders, and meta-train this inference model alongside the policy. We show empirically that our approach outperforms existing methods that use a model-free approach, sample from the approximate posterior, maintain memory-free models of others, or do not fully utilise the known structure of the environment.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Machine Learning,Computer Science - Multiagent Systems},
  file = {/home/jackyansongli/Zotero/storage/DGEJKVC8/Zintgraf et al. - 2022 - Deep Interactive Bayesian Reinforcement Learning v.pdf;/home/jackyansongli/Zotero/storage/HDI8WMDQ/2101.html}
}
